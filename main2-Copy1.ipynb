{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2c7375-612f-4b3d-9f0c-a187a5eea9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "The vocabulary is: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' '] (size =27)\n",
      "CTC Blank Index: 0\n",
      "Label Padding Value: 0\n",
      "Using dlib predictor: shape_predictor_68_face_landmarks.dat\n",
      "Searching for video files...\n",
      "Found 1000 files for speaker 1\n",
      "Found 1000 files for speaker 2\n",
      "Using overlapped speakers split. 200 test sentences per speaker.\n",
      "Total video files found: 2000\n",
      "Training files: 1600\n",
      "Test files: 400\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional\n",
    "import imageio\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 5)\n",
    "\n",
    "# --- Configuration ---\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "FRAME_COUNT = 75\n",
    "FRAME_HEIGHT = 50\n",
    "FRAME_WIDTH = 100\n",
    "FRAME_CHANNELS = 3\n",
    "DROPOUT_P = 0.5\n",
    "\n",
    "# --- Kaggle Paths ---\n",
    "KAGGLE_BASE_PATH = './GRIDCorpus/data/'\n",
    "# BASE_VIDEO_DIR = os.path.join(KAGGLE_BASE_PATH) # Root for s*_processed folders\n",
    "# BASE_ALIGN_DIR = os.path.join(KAGGLE_BASE_PATH) # Alignments are inside s*_processed/align\n",
    "\n",
    "# --- Local Paths (if testing locally, adjust these) ---\n",
    "# KAGGLE_BASE_PATH = './data/' # Example if 'data' dir is local\n",
    "\n",
    "# Assuming speaker IDs 1-34, excluding 21\n",
    "# Reduced speaker list for faster testing initially - ADJUST AS NEEDED\n",
    "# ALL_SPEAKER_IDS = [f's{i}' for i in range(1, 3) if i != 21] # Example: Use only s1, s2\n",
    "ALL_SPEAKER_IDS = [f's{i}' for i in range(1, 3) if i != 21] # Full speaker list\n",
    "\n",
    "# --- Split Configuration ---\n",
    "TEST_SPEAKERS_UNSEEN = ['s1', 's2'] # Adjust if needed based on speakers used\n",
    "NUM_TEST_SENTENCES_OVERLAPPED = 200\n",
    "SPLIT_MODE = 'overlapped' # Choose 'unseen' or 'overlapped'\n",
    "\n",
    "# Normalization constants\n",
    "NORM_MEAN = np.array([0.7136, 0.4906, 0.3283], dtype=np.float32)\n",
    "NORM_STD = np.array([0.1138, 0.1078, 0.0917], dtype=np.float32)\n",
    "\n",
    "# --- Vocabulary ---\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz \"]\n",
    "char_to_num_dict = {char: i + 1 for i, char in enumerate(vocab)} # Start indices from 1\n",
    "num_to_char_dict = {i + 1: char for i, char in enumerate(vocab)}\n",
    "VOCAB_SIZE = len(vocab)\n",
    "CTC_BLANK_INDEX = 0\n",
    "LABEL_PADDING_VALUE = CTC_BLANK_INDEX\n",
    "\n",
    "print(f\"The vocabulary is: {vocab} (size ={VOCAB_SIZE})\")\n",
    "print(f\"CTC Blank Index: {CTC_BLANK_INDEX}\")\n",
    "print(f\"Label Padding Value: {LABEL_PADDING_VALUE}\")\n",
    "\n",
    "# --- Mouth Extraction (dlib setup) ---\n",
    "try:\n",
    "    # Try loading from Kaggle input first, then local\n",
    "    DLIB_PREDICTOR_PATHS = [\n",
    "        \"shape_predictor_68_face_landmarks.dat\"\n",
    "    ]\n",
    "    DLIB_LANDMARK_PREDICTOR = None\n",
    "    for path in DLIB_PREDICTOR_PATHS:\n",
    "        if os.path.exists(path):\n",
    "            DLIB_LANDMARK_PREDICTOR = path\n",
    "            break\n",
    "    if DLIB_LANDMARK_PREDICTOR is None:\n",
    "        raise FileNotFoundError(\"dlib landmark predictor not found.\")\n",
    "\n",
    "    print(f\"Using dlib predictor: {DLIB_LANDMARK_PREDICTOR}\")\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(DLIB_LANDMARK_PREDICTOR)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing dlib: {e}\")\n",
    "    print(\"Make sure dlib is installed and the predictor file exists (shape_predictor_68_face_landmarks.dat).\")\n",
    "    exit()\n",
    "\n",
    "def extract_mouth_region(frame: np.ndarray) -> Optional[np.ndarray]:\n",
    "    # (Keep the function from previous version - it operates on a single frame)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    face = faces[0]\n",
    "    landmarks = predictor(gray, face)\n",
    "    points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)])\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(points)\n",
    "    # Adjust margins carefully - trying to match TF code's effective crop\n",
    "    y_start = max(y + 15 - 30, 0)\n",
    "    y_end = y + 15 + h + 30\n",
    "    x_start = max(x + 15 - 30, 0)\n",
    "    x_end = x + 15 + w + 30\n",
    "\n",
    "    y_start = max(0, y_start)\n",
    "    y_end = min(frame.shape[0], y_end)\n",
    "    x_start = max(0, x_start)\n",
    "    x_end = min(frame.shape[1], x_end)\n",
    "\n",
    "    cropped = frame[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    if cropped.size == 0:\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "    try:\n",
    "        cropped = cv2.resize(cropped, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "        return cropped\n",
    "    except cv2.error as e:\n",
    "        print(f\"Warning: cv2.resize error ({e}). Returning zero frame.\")\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "# --- Alignment Loading (Needs correct path derivation) ---\n",
    "def get_align_path(video_path):\n",
    "    \"\"\"Derives the alignment file path from the video path for Kaggle structure.\"\"\"\n",
    "    parts = video_path.split(os.path.sep)\n",
    "    video_filename = parts[-1]\n",
    "    speaker_processed_dir = parts[-2] # e.g., 's1_processed'\n",
    "    base_name = os.path.splitext(video_filename)[0]\n",
    "    # Construct align path relative to the input directory base\n",
    "    align_dir = os.path.join(KAGGLE_BASE_PATH, speaker_processed_dir, 'align')\n",
    "    return os.path.join(align_dir, f'{base_name}.align')\n",
    "\n",
    "def load_alignments(align_file: str) -> Optional[List[str]]:\n",
    "    # (Keep function from previous version, path is handled outside now)\n",
    "    alignments_chars = []\n",
    "    try:\n",
    "        with open(align_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                _, _, token = parts\n",
    "                if token != 'sil':\n",
    "                    token_characters = list(token.lower() + ' ')\n",
    "                    alignments_chars.extend(token_characters)\n",
    "        return alignments_chars[:-1] if alignments_chars else []\n",
    "    except FileNotFoundError:\n",
    "         # print(f\"Warning: Alignment file not found: {align_file}\") # Less noisy\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading alignments {align_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- File Listing and Splitting ---\n",
    "all_video_files = []\n",
    "print(\"Searching for video files...\")\n",
    "# Search for common video formats within s*_processed directories\n",
    "for speaker_id_num in range(1, 3): # Check all potential speaker numbers\n",
    "    if speaker_id_num == 21: continue # Skip speaker 21\n",
    "    speaker_processed_folder = f\"s{speaker_id_num}_processed\"\n",
    "    speaker_path_pattern = os.path.join(KAGGLE_BASE_PATH, speaker_processed_folder, '*.mpg') # Check mpg first\n",
    "    files = glob.glob(speaker_path_pattern)\n",
    "    if not files:\n",
    "        speaker_path_pattern = os.path.join(KAGGLE_BASE_PATH, speaker_processed_folder, '*.mp4') # Check mp4\n",
    "        files = glob.glob(speaker_path_pattern)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"Warning: No .mpg or .mp4 files found for speaker {speaker_id_num} in {os.path.join(KAGGLE_BASE_PATH, speaker_processed_folder)}\")\n",
    "    else:\n",
    "        print(f\"Found {len(files)} files for speaker {speaker_id_num}\")\n",
    "    all_video_files.extend(files)\n",
    "\n",
    "\n",
    "if not all_video_files:\n",
    "    raise FileNotFoundError(f\"No video files (.mpg or .mp4) found in subdirectories like {os.path.join(KAGGLE_BASE_PATH, 's*_processed/')}. Please check KAGGLE_BASE_PATH and dataset structure.\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(all_video_files)\n",
    "train_files, test_files = [], []\n",
    "\n",
    "# --- Splitting Logic (based on video paths now) ---\n",
    "if SPLIT_MODE == 'unseen':\n",
    "    print(f\"Using unseen speakers split. Test speakers: {TEST_SPEAKERS_UNSEEN}\")\n",
    "    for f in all_video_files:\n",
    "        speaker_id = os.path.basename(os.path.dirname(f)).split('_')[0] # Extract 'sX' from 'sX_processed'\n",
    "        if speaker_id in TEST_SPEAKERS_UNSEEN:\n",
    "            test_files.append(f)\n",
    "        else:\n",
    "            train_files.append(f)\n",
    "elif SPLIT_MODE == 'overlapped':\n",
    "    print(f\"Using overlapped speakers split. {NUM_TEST_SENTENCES_OVERLAPPED} test sentences per speaker.\")\n",
    "    files_by_speaker = {}\n",
    "    for f in all_video_files:\n",
    "        speaker_id = os.path.basename(os.path.dirname(f)).split('_')[0]\n",
    "        if speaker_id not in files_by_speaker:\n",
    "            files_by_speaker[speaker_id] = []\n",
    "        files_by_speaker[speaker_id].append(f)\n",
    "\n",
    "    for speaker_id, files in files_by_speaker.items():\n",
    "        np.random.shuffle(files)\n",
    "        test_count = min(NUM_TEST_SENTENCES_OVERLAPPED, len(files))\n",
    "        test_files.extend(files[:test_count])\n",
    "        train_files.extend(files[test_count:])\n",
    "else:\n",
    "    raise ValueError(\"Invalid SPLIT_MODE. Choose 'unseen' or 'overlapped'.\")\n",
    "\n",
    "print(f\"Total video files found: {len(all_video_files)}\")\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")\n",
    "\n",
    "if not train_files or not test_files:\n",
    "    raise ValueError(\"Training or test set is empty. Check file paths and splitting logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef998c9-fd6c-418f-b609-de64554ceb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch Dataset (On-the-fly Preprocessing) ---\n",
    "class GRIDDataset(Dataset):\n",
    "    def __init__(self, file_paths: List[str], augment: bool = False):\n",
    "        self.file_paths = file_paths\n",
    "        self.augment = augment\n",
    "        self.target_frame_count = FRAME_COUNT\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.file_paths[idx]\n",
    "        try:\n",
    "            # --- Load Original Video ---\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Warning: Could not open video {video_path}. Skipping.\")\n",
    "                raise IOError(\"Could not open video\")\n",
    "\n",
    "            extracted_mouth_frames = []\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                mouth_crop = extract_mouth_region(frame) # Returns uint8 BGR\n",
    "                if mouth_crop is None:\n",
    "                     # Use a black frame if detection fails\n",
    "                     mouth_crop = np.zeros((FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=np.uint8)\n",
    "                extracted_mouth_frames.append(mouth_crop)\n",
    "            cap.release()\n",
    "\n",
    "            if not extracted_mouth_frames:\n",
    "                 print(f\"Warning: No frames extracted from {video_path}. Skipping.\")\n",
    "                 raise ValueError(\"No frames extracted\")\n",
    "\n",
    "            # --- Pad/Truncate Frames ---\n",
    "            frames_np_uint8 = np.stack(extracted_mouth_frames)\n",
    "            current_frame_count = frames_np_uint8.shape[0]\n",
    "\n",
    "            if current_frame_count != self.target_frame_count:\n",
    "                if current_frame_count > self.target_frame_count:\n",
    "                    frames_np_uint8 = frames_np_uint8[:self.target_frame_count, ...]\n",
    "                else:\n",
    "                    pad_width = ((0, self.target_frame_count - current_frame_count), (0, 0), (0, 0), (0, 0))\n",
    "                    frames_np_uint8 = np.pad(frames_np_uint8, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "            # --- Convert, Normalize, BGR->RGB ---\n",
    "            frames_float = frames_np_uint8.astype(np.float32) / 255.0\n",
    "            frames_rgb = frames_float[..., ::-1]\n",
    "            frames_normalized = (frames_rgb - NORM_MEAN) / NORM_STD\n",
    "            frames_tensor = torch.tensor(frames_normalized, dtype=torch.float32)\n",
    "\n",
    "            # --- Get Alignments ---\n",
    "            align_path = get_align_path(video_path)\n",
    "            alignments_list = load_alignments(align_path)\n",
    "            if alignments_list is None:\n",
    "                print(f\"Skipping {video_path} due to missing alignment {align_path}.\")\n",
    "                raise ValueError(\"Missing alignment\")\n",
    "\n",
    "            label_indices = [char_to_num_dict.get(char, CTC_BLANK_INDEX) for char in alignments_list]\n",
    "            label_tensor = torch.tensor(label_indices, dtype=torch.long)\n",
    "\n",
    "            # --- Augmentation ---\n",
    "            if self.augment and torch.rand(1).item() > 0.5:\n",
    "                frames_tensor = torch.flip(frames_tensor, dims=[2]) # Flip width\n",
    "\n",
    "            return frames_tensor, label_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {idx} ({video_path}): {e}\")\n",
    "            # Return dummy data that collate_fn can filter\n",
    "            return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                   torch.tensor([], dtype=torch.long)\n",
    "\n",
    "\n",
    "# --- Collate Function (Remains the same) ---\n",
    "def collate_fn(batch):\n",
    "    batch = [(frames, labels) for frames, labels in batch if frames is not None and labels.numel() > 0]\n",
    "    if not batch:\n",
    "        return torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "\n",
    "    frames_list, labels_list = zip(*batch)\n",
    "    frames_batch = torch.stack(frames_list, dim=0)\n",
    "    label_lengths = torch.tensor([len(lbl) for lbl in labels_list], dtype=torch.long)\n",
    "    labels_padded = pad_sequence(labels_list, batch_first=True, padding_value=LABEL_PADDING_VALUE)\n",
    "    input_lengths = torch.full(size=(len(batch),), fill_value=FRAME_COUNT, dtype=torch.long)\n",
    "    return frames_batch, labels_padded, input_lengths, label_lengths\n",
    "\n",
    "\n",
    "# --- Data Loaders ---\n",
    "BATCH_SIZE_TRAIN = 32 # Adjust based on Kaggle GPU memory (T4/P100)\n",
    "BATCH_SIZE_TEST = 32\n",
    "NUM_WORKERS = 2 # Use multiple workers on Kaggle\n",
    "\n",
    "train_dataset = GRIDDataset(train_files, augment=True)\n",
    "test_dataset = GRIDDataset(test_files, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                          persistent_workers=(NUM_WORKERS > 0)) # Add persistent_workers\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False,\n",
    "                         collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                         persistent_workers=(NUM_WORKERS > 0)) # Add persistent_workers\n",
    "\n",
    "print(\"DataLoaders created.\")\n",
    "\n",
    "\n",
    "# --- PyTorch LipNet Model (Remains the same as previous version) ---\n",
    "class LipNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_p=DROPOUT_P):\n",
    "        super(LipNet, self).__init__()\n",
    "        self.num_classes = num_classes # Should include blank token\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Spatiotemporal Convolutional Layers (STCNN)\n",
    "        self.conv1 = nn.Conv3d(FRAME_CHANNELS, 32, kernel_size=(3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop1 = nn.Dropout3d(dropout_p)\n",
    "\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop2 = nn.Dropout3d(dropout_p)\n",
    "\n",
    "        self.conv3 = nn.Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.bn3 = nn.BatchNorm3d(96)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop3 = nn.Dropout3d(dropout_p)\n",
    "\n",
    "        self.rnn_input_size = 96 * 3 * 6\n",
    "\n",
    "        # Bidirectional GRU Layers\n",
    "        self.gru1 = nn.GRU(self.rnn_input_size, 256, bidirectional=True, batch_first=True)\n",
    "        self.drop_gru1 = nn.Dropout(dropout_p)\n",
    "        self.gru2 = nn.GRU(256 * 2, 256, bidirectional=True, batch_first=True)\n",
    "        self.drop_gru2 = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(256 * 2, self.num_classes)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None: init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d): init.constant_(m.weight, 1); init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name: init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name: init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name: init.constant_(param.data, 0)\n",
    "            elif isinstance(m, nn.Linear): init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu'); init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (N, T, H, W, C)\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous() # -> (N, C, T, H, W)\n",
    "        x = self.conv1(x); x = self.bn1(x); x = F.relu(x); x = self.pool1(x); x = self.drop1(x)\n",
    "        x = self.conv2(x); x = self.bn2(x); x = F.relu(x); x = self.pool2(x); x = self.drop2(x)\n",
    "        x = self.conv3(x); x = self.bn3(x); x = F.relu(x); x = self.pool3(x); x = self.drop3(x)\n",
    "        N, C, T, H, W = x.size()\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous() # -> (N, T, C, H, W)\n",
    "        x = x.view(N, T, -1) # -> (N, T, C*H*W)\n",
    "        x, _ = self.gru1(x); x = self.drop_gru1(x)\n",
    "        x, _ = self.gru2(x); x = self.drop_gru2(x) # -> (N, T, hidden*2)\n",
    "        x = self.fc(x) # -> (N, T, num_classes)\n",
    "        x = x.permute(1, 0, 2).contiguous() # -> (T, N, C) for CTC\n",
    "        log_probs = F.log_softmax(x, dim=2)\n",
    "        return log_probs\n",
    "\n",
    "# --- Instantiate Model, Loss, Optimizer (Remain the same) ---\n",
    "model = LipNet(num_classes=VOCAB_SIZE + 1).to(DEVICE)\n",
    "ctc_loss = nn.CTCLoss(blank=CTC_BLANK_INDEX, reduction='mean', zero_infinity=True)\n",
    "LEARNING_RATE = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True)\n",
    "\n",
    "# --- Checkpoint Path (Use Kaggle working directory) ---\n",
    "checkpoint_dir = './models_pytorch/main2_Copy/'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'lipnet_checkpoint.pth')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# --- Greedy Decoder & ProduceExample (Remain the same) ---\n",
    "def greedy_decoder(log_probs, input_lengths):\n",
    "    # (Keep function from previous version)\n",
    "    decoded_sequences = []\n",
    "    for i in range(log_probs.size(1)):\n",
    "        sample_log_probs = log_probs[:input_lengths[i], i, :]\n",
    "        best_path = torch.argmax(sample_log_probs, dim=1)\n",
    "        decoded = []\n",
    "        last_char = -1\n",
    "        for char_idx in best_path:\n",
    "            idx = char_idx.item()\n",
    "            if idx != last_char and idx != CTC_BLANK_INDEX:\n",
    "                decoded.append(idx)\n",
    "            if idx != CTC_BLANK_INDEX:\n",
    "                last_char = idx\n",
    "        decoded_sequences.append(decoded)\n",
    "    return decoded_sequences\n",
    "\n",
    "def produce_example(model, dataset_loader, num_to_char_map):\n",
    "    # (Keep function from previous version)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            frames_batch, labels_batch, input_lengths, label_lengths = next(iter(dataset_loader))\n",
    "            frames_batch = frames_batch.to(DEVICE)\n",
    "            log_probs = model(frames_batch)\n",
    "            log_probs_cpu = log_probs.cpu()\n",
    "            input_lengths_cpu = input_lengths.cpu()\n",
    "            decoded_indices_list = greedy_decoder(log_probs_cpu, input_lengths_cpu)\n",
    "            print(\"\\n--- Example Predictions ---\")\n",
    "            N_EXAMPLES_TO_SHOW = min(2, frames_batch.size(0))\n",
    "            for i in range(N_EXAMPLES_TO_SHOW):\n",
    "                original_indices = labels_batch[i][:label_lengths[i]].tolist()\n",
    "                original_text = \"\".join([num_to_char_map.get(idx, '?') for idx in original_indices])\n",
    "                print(f'Original:     {original_text}')\n",
    "                prediction_indices = decoded_indices_list[i]\n",
    "                print(f'Filtered Idx: {prediction_indices}')\n",
    "                prediction_text = \"\".join([num_to_char_map.get(idx, '?') for idx in prediction_indices])\n",
    "                print(f'Prediction:   {prediction_text}')\n",
    "                print('-'*50)\n",
    "            print(\"--- End Examples ---\\n\")\n",
    "        except StopIteration: print(\"Warning: Could not get batch for ProduceExample.\")\n",
    "        except Exception as e: print(f\"Error during ProduceExample: {e}\")\n",
    "    model.train()\n",
    "\n",
    "# --- Training Loop (Remains largely the same, ensure data paths are correct) ---\n",
    "EPOCHS = 50\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['loss']\n",
    "        # Load scheduler state if available\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "             scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(f\"Resuming training from epoch {start_epoch}, Best loss: {best_val_loss:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
    "        start_epoch = 0\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "# Estimate batches if needed (DataLoader length should work)\n",
    "try:\n",
    "    num_train_batches = len(train_loader)\n",
    "    num_test_batches = len(test_loader)\n",
    "except TypeError: # Handle cases where len() isn't directly supported\n",
    "    print(\"Could not determine DataLoader length directly, estimating batches.\")\n",
    "    num_train_batches = len(train_files) // BATCH_SIZE_TRAIN\n",
    "    num_test_batches = len(test_files) // BATCH_SIZE_TEST\n",
    "\n",
    "\n",
    "print(f\"Training batches per epoch: {num_train_batches}\")\n",
    "print(f\"Validation batches per epoch: {num_test_batches}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_pbar = tqdm(enumerate(train_loader), total=num_train_batches, desc=f\"Epoch {epoch+1} Train\")\n",
    "\n",
    "    for i, batch_data in train_pbar:\n",
    "        # Ensure batch is not empty (can happen if all samples failed in collate)\n",
    "        if batch_data[0].numel() == 0:\n",
    "             print(f\"Warning: Skipping empty batch at step {i}\")\n",
    "             continue\n",
    "\n",
    "        frames_batch, labels_padded, input_lengths, label_lengths = batch_data\n",
    "        frames_batch = frames_batch.to(DEVICE)\n",
    "        labels_padded = labels_padded.to(DEVICE)\n",
    "        input_lengths = input_lengths.to(DEVICE)\n",
    "        label_lengths = label_lengths.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(frames_batch) # T, N, C\n",
    "        T = log_probs.size(0)\n",
    "        input_lengths_clamped = torch.clamp(input_lengths, max=T).cpu() # Ensure lengths are on CPU for loss\n",
    "        label_lengths_cpu = label_lengths.cpu()\n",
    "        labels_padded_cpu = labels_padded.cpu()\n",
    "\n",
    "        loss = ctc_loss(log_probs.cpu(), labels_padded_cpu, input_lengths_clamped, label_lengths_cpu)\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "             print(f\"\\nWarning: NaN or Inf loss detected at batch {i}. Skipping batch.\")\n",
    "             continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_pbar.set_postfix({'loss': running_loss / (i + 1)})\n",
    "\n",
    "    avg_train_loss = running_loss / num_train_batches if num_train_batches > 0 else 0\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_pbar = tqdm(enumerate(test_loader), total=num_test_batches, desc=f\"Epoch {epoch+1} Val\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in val_pbar:\n",
    "            if batch_data[0].numel() == 0: continue # Skip empty batch\n",
    "\n",
    "            frames_batch, labels_padded, input_lengths, label_lengths = batch_data\n",
    "            frames_batch = frames_batch.to(DEVICE)\n",
    "            labels_padded = labels_padded.to(DEVICE)\n",
    "            input_lengths = input_lengths.to(DEVICE)\n",
    "            label_lengths = label_lengths.to(DEVICE)\n",
    "\n",
    "            log_probs = model(frames_batch)\n",
    "            T = log_probs.size(0)\n",
    "            input_lengths_clamped = torch.clamp(input_lengths, max=T).cpu()\n",
    "            label_lengths_cpu = label_lengths.cpu()\n",
    "            labels_padded_cpu = labels_padded.cpu()\n",
    "\n",
    "            loss = ctc_loss(log_probs.cpu(), labels_padded_cpu, input_lengths_clamped, label_lengths_cpu)\n",
    "\n",
    "            if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                running_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'val_loss': running_val_loss / (i + 1)})\n",
    "\n",
    "    avg_val_loss = running_val_loss / num_test_batches if num_test_batches > 0 else 0\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "    scheduler.step(avg_val_loss) # Step the scheduler\n",
    "\n",
    "    # --- Save Checkpoint ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}, saving model to {checkpoint_path}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(), # Save scheduler state\n",
    "            'loss': avg_val_loss,\n",
    "            'train_losses': train_losses, # Save history\n",
    "            'val_losses': val_losses,     # Save history\n",
    "        }, checkpoint_path)\n",
    "        best_val_loss = avg_val_loss\n",
    "    else:\n",
    "        print(f\"Validation loss ({avg_val_loss:.4f}) did not improve from {best_val_loss:.4f}\")\n",
    "\n",
    "    # --- Show Examples periodically ---\n",
    "    if (epoch + 1) % 5 == 0 or epoch == EPOCHS - 1:\n",
    "         produce_example(model, test_loader, num_to_char_dict)\n",
    "\n",
    "print(\"\\nTraining Finished.\")\n",
    "\n",
    "# --- Plotting (Remains the same) ---\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, color='red', label=\"Training Loss\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, color='green', label=\"Validation Loss\")\n",
    "plt.title(\"Training/Validation Loss vs Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/kaggle/working/loss_curve_pytorch.png\") # Save to working dir\n",
    "plt.show()\n",
    "\n",
    "# --- Final Testing Example (Remains the same) ---\n",
    "print(\"\\n--- Running Final Test Examples ---\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading best weights from {checkpoint_path} for final test...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Best weights loaded successfully.\")\n",
    "        produce_example(model, test_loader, num_to_char_dict)\n",
    "    except Exception as e: print(f\"Error loading best weights: {e}\")\n",
    "else: print(\"No checkpoint found for final testing.\")\n",
    "print(\"--- Final Testing Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692bfbb-4901-4294-97be-a8e79af0b060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72067b-93e3-43c5-b250-d190fe2d96d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ed5bb-6541-468b-b6aa-96bf923c65cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6baee-69cf-41f1-9a68-b9eba6769362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd7a04-3efb-48ef-a976-d1332741972b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb0fae-91d3-498c-9ec3-cdb707e4a881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469af363-1230-4208-a5b6-cfcb61728a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eab775-2a7a-4cbf-bd9f-81f8451f0d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e76326-6358-4883-9061-b45ddbc8a11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ab55d-c219-43df-a0ae-0fc8a0e0156c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51bd0c-f306-464d-9bbc-e494c450dc20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5bfec-394e-4135-8f27-b27c1ccee467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092afe9-7169-4b52-bc34-881352c9da18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf8b369-4c32-4436-b361-58c60ac52760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mouth Extraction (Mostly unchanged, ensure return type is numpy) ---\n",
    "try:\n",
    "    DLIB_LANDMARK_PREDICTOR = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    if not os.path.exists(DLIB_LANDMARK_PREDICTOR):\n",
    "        # Add download/unzip logic here if needed, e.g., using requests/bz2\n",
    "        print(f\"Error: dlib landmark predictor '{DLIB_LANDMARK_PREDICTOR}' not found.\")\n",
    "        print(\"Please download it from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "        exit()\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(DLIB_LANDMARK_PREDICTOR)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing dlib: {e}\")\n",
    "    print(\"Make sure dlib is installed correctly and the predictor file exists.\")\n",
    "    exit()\n",
    "\n",
    "def extract_mouth_region(frame: np.ndarray) -> Optional[np.ndarray]:\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    face = faces[0]\n",
    "    landmarks = predictor(gray, face)\n",
    "    points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)])\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(points)\n",
    "    # Adjust margins carefully - TF code used slightly different logic\n",
    "    # Let's try to match the TF code's effective crop:\n",
    "    y_start = max(y + 15 - 30, 0) # y + 15 was start, margin was 30\n",
    "    y_end = y + 15 + h + 30\n",
    "    x_start = max(x + 15 - 30, 0) # x + 15 was start, margin was 30\n",
    "    x_end = x + 15 + w + 30\n",
    "\n",
    "    # Ensure coordinates are within frame bounds\n",
    "    y_start = max(0, y_start)\n",
    "    y_end = min(frame.shape[0], y_end)\n",
    "    x_start = max(0, x_start)\n",
    "    x_end = min(frame.shape[1], x_end)\n",
    "\n",
    "    cropped = frame[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    if cropped.size == 0: # Handle empty crop\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "    try:\n",
    "        cropped = cv2.resize(cropped, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "        return cropped\n",
    "    except cv2.error as e:\n",
    "        print(f\"Warning: cv2.resize error ({e}). Returning zero frame.\")\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "def process_and_save_video(video_path: str, output_dir: str) -> None:\n",
    "    \"\"\"Processes a single video, extracts mouth regions, and saves as .npy\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening video file: {video_path}\")\n",
    "            return None\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            mouth_frame = extract_mouth_region(frame)\n",
    "            if mouth_frame is not None:\n",
    "                frames.append(mouth_frame)\n",
    "            else:\n",
    "                # If detection fails, append a zero frame\n",
    "                frames.append(np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8))\n",
    "        cap.release()\n",
    "\n",
    "        if not frames:\n",
    "            print(f\"Warning: No frames extracted from {video_path}\")\n",
    "            return None\n",
    "\n",
    "        # Ensure video has FRAME_COUNT frames (pad/truncate if needed)\n",
    "        frames_np = np.array(frames, dtype=np.uint8) # Keep as uint8 for now\n",
    "        current_frame_count = frames_np.shape[0]\n",
    "        if current_frame_count != FRAME_COUNT:\n",
    "             if current_frame_count > FRAME_COUNT:\n",
    "                 frames_np = frames_np[:FRAME_COUNT, ...]\n",
    "             else:\n",
    "                 pad_width = ((0, FRAME_COUNT - current_frame_count), (0, 0), (0, 0), (0, 0))\n",
    "                 frames_np = np.pad(frames_np, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "        video_id = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        output_path = os.path.join(output_dir, f\"{video_id}_mouth.npy\")\n",
    "        np.save(output_path, frames_np)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Function to preprocess all videos (Run only once) ---\n",
    "def preprocess_all_videos(force_reprocess=False):\n",
    "    print(\"Starting video preprocessing...\")\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    for speaker_id in ALL_SPEAKER_IDS:\n",
    "        input_videos_dir = os.path.join('./data/', speaker_id, 'video') # Assuming video folder structure\n",
    "        output_preprocessed_dir = os.path.join(BASE_PROCESSED_PATH, speaker_id)\n",
    "        os.makedirs(output_preprocessed_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing speaker: {speaker_id}\")\n",
    "        video_files = glob.glob(os.path.join(input_videos_dir, '*.mpg')) # Assuming .mpg format\n",
    "\n",
    "        if not video_files:\n",
    "             print(f\"  Warning: No .mpg files found in {input_videos_dir}\")\n",
    "             continue\n",
    "\n",
    "        for video_file in tqdm(video_files, desc=f\"Speaker {speaker_id}\", unit=\"video\"):\n",
    "             video_id = os.path.splitext(os.path.basename(video_file))[0]\n",
    "             output_path = os.path.join(output_preprocessed_dir, f\"{video_id}_mouth.npy\")\n",
    "             if not force_reprocess and os.path.exists(output_path):\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "             process_and_save_video(video_file, output_preprocessed_dir)\n",
    "             processed_count += 1\n",
    "\n",
    "    print(f\"\\nPreprocessing finished. Processed: {processed_count}, Skipped (already exists): {skipped_count}\")\n",
    "\n",
    "# --- UNCOMMENT AND RUN THIS ONCE TO PREPROCESS ---\n",
    "# preprocess_all_videos(force_reprocess=False)\n",
    "# print(\"Preprocessing complete. You can now comment out the preprocess_all_videos call.\")\n",
    "# --- END PREPROCESSING CALL ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3473b7c4-e4a9-43b8-9ef0-40424b9e6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GIF Creation (Unchanged) ---\n",
    "def create_gif_from_npy(npy_path: str, gif_path: str, duration: float = 0.04) -> None:\n",
    "    try:\n",
    "        frames = np.load(npy_path) # Should be uint8\n",
    "        imageio.mimsave(gif_path, frames, duration=duration)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Npy file not found at {npy_path}, cannot create GIF.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating GIF from {npy_path}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8424bde4-0652-4476-a445-3757e6e72fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Alignment Loading (Unchanged, returns list of chars) ---\n",
    "def load_alignments(align_file: str) -> Optional[List[str]]:\n",
    "    alignments_chars = []\n",
    "    try:\n",
    "        with open(align_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                _, _, token = parts\n",
    "                if token != 'sil':\n",
    "                    token_characters = list(token.lower() + ' ')\n",
    "                    alignments_chars.extend(token_characters)\n",
    "        # Remove the trailing space from the last word\n",
    "        return alignments_chars[:-1] if alignments_chars else []\n",
    "    except FileNotFoundError:\n",
    "         print(f\"Warning: Alignment file not found: {align_file}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading alignments {align_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14035da-9807-462e-9066-c4aca1bb0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'a', 'y', ' ', 'r', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'z', ' ', 'f', 'o', 'u', 'r', ' ', 'p', 'l', 'e', 'a', 's', 'e']\n"
     ]
    }
   ],
   "source": [
    "example_alignment = load_alignments(\"./data/alignments/s1/lrwz4p.align\")\n",
    "print(example_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338b18fd-fec0-4a19-99ce-7fdbd8eb4e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using overlapped speakers split. 200 test sentences per speaker.\n",
      "Total files found: 995\n",
      "Training files: 795\n",
      "Test files: 200\n"
     ]
    }
   ],
   "source": [
    "# --- File Listing and Splitting ---\n",
    "all_npy_files = []\n",
    "for speaker_id in ALL_SPEAKER_IDS:\n",
    "    speaker_path = os.path.join(BASE_PROCESSED_PATH, speaker_id, '*.npy')\n",
    "    files = glob.glob(speaker_path)\n",
    "    if not files:\n",
    "        print(f\"Warning: No .npy files found for speaker {speaker_id} in {os.path.join(BASE_PROCESSED_PATH, speaker_id)}\")\n",
    "    all_npy_files.extend(files)\n",
    "\n",
    "if not all_npy_files:\n",
    "    raise FileNotFoundError(f\"No .npy files found in {BASE_PROCESSED_PATH} for speakers {ALL_SPEAKER_IDS}. Did preprocessing run?\")\n",
    "\n",
    "np.random.shuffle(all_npy_files)\n",
    "train_files, test_files = [], []\n",
    "\n",
    "if SPLIT_MODE == 'unseen':\n",
    "    print(f\"Using unseen speakers split. Test speakers: {TEST_SPEAKERS_UNSEEN}\")\n",
    "    for f in all_npy_files:\n",
    "        speaker_id = os.path.basename(os.path.dirname(f))\n",
    "        if speaker_id in TEST_SPEAKERS_UNSEEN:\n",
    "            test_files.append(f)\n",
    "        else:\n",
    "            train_files.append(f)\n",
    "elif SPLIT_MODE == 'overlapped':\n",
    "    print(f\"Using overlapped speakers split. {NUM_TEST_SENTENCES_OVERLAPPED} test sentences per speaker.\")\n",
    "    files_by_speaker = {}\n",
    "    for f in all_npy_files:\n",
    "        speaker_id = os.path.basename(os.path.dirname(f))\n",
    "        if speaker_id not in files_by_speaker:\n",
    "            files_by_speaker[speaker_id] = []\n",
    "        files_by_speaker[speaker_id].append(f)\n",
    "\n",
    "    for speaker_id, files in files_by_speaker.items():\n",
    "        np.random.shuffle(files)\n",
    "        test_count = min(NUM_TEST_SENTENCES_OVERLAPPED, len(files))\n",
    "        test_files.extend(files[:test_count])\n",
    "        train_files.extend(files[test_count:])\n",
    "else:\n",
    "    raise ValueError(\"Invalid SPLIT_MODE. Choose 'unseen' or 'overlapped'.\")\n",
    "\n",
    "print(f\"Total files found: {len(all_npy_files)}\")\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")\n",
    "\n",
    "if not train_files or not test_files:\n",
    "    raise ValueError(\"Training or test set is empty. Check file paths and splitting logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "834ccdb7-5ff3-44e1-a7d0-b3411f81f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch Dataset ---\n",
    "class GRIDDataset(Dataset):\n",
    "    def __init__(self, file_paths: List[str], augment: bool = False):\n",
    "        self.file_paths = file_paths\n",
    "        self.augment = augment\n",
    "        self.target_frame_count = FRAME_COUNT # Define target length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        npy_path = self.file_paths[idx]\n",
    "        try:\n",
    "            # --- Load Frames (Keep as uint8 for now) ---\n",
    "            # This part loads the preprocessed mouth crops saved earlier\n",
    "            frames_uint8 = np.load(npy_path)\n",
    "\n",
    "            if frames_uint8.ndim != 4 or frames_uint8.shape[1:] != (FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS):\n",
    "                 print(f\"Warning: Unexpected shape {frames_uint8.shape} for {npy_path}. Skipping.\")\n",
    "                 # Return dummy data that collate_fn can handle/filter\n",
    "                 return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                        torch.tensor([], dtype=torch.long)\n",
    "\n",
    "            # --- Ensure Correct Frame Count ---\n",
    "            current_frame_count = frames_uint8.shape[0]\n",
    "            if current_frame_count != self.target_frame_count:\n",
    "                if current_frame_count > self.target_frame_count:\n",
    "                    # Truncate\n",
    "                    frames_uint8 = frames_uint8[:self.target_frame_count, ...]\n",
    "                else:\n",
    "                    # Pad with zeros\n",
    "                    pad_width = ((0, self.target_frame_count - current_frame_count), (0, 0), (0, 0), (0, 0))\n",
    "                    # Ensure padding uses the correct dtype (uint8) before conversion\n",
    "                    frames_uint8 = np.pad(frames_uint8, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "            # --- Convert to Float, Normalize, Convert BGR->RGB ---\n",
    "            frames_float = frames_uint8.astype(np.float32) / 255.0\n",
    "            frames_rgb = frames_float[..., ::-1] # BGR to RGB\n",
    "            frames_normalized = (frames_rgb - NORM_MEAN) / NORM_STD\n",
    "            frames_tensor = torch.tensor(frames_normalized, dtype=torch.float32)\n",
    "            # Shape should now be guaranteed [75, 50, 100, 3]\n",
    "\n",
    "            # --- Get Alignments ---\n",
    "            # (Rest of your alignment loading logic remains the same)\n",
    "            parts = npy_path.split(os.path.sep)\n",
    "            speaker_id = parts[-2]\n",
    "            base_name = os.path.splitext(parts[-1])[0]\n",
    "            if base_name.endswith('_mouth'):\n",
    "                base_name = base_name[:-6]\n",
    "            align_file = os.path.join(BASE_ALIGN_PATH, speaker_id, f'{base_name}.align')\n",
    "            alignments_list = load_alignments(align_file)\n",
    "\n",
    "            if alignments_list is None:\n",
    "                print(f\"Skipping {npy_path} due to missing alignment.\")\n",
    "                return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                       torch.tensor([], dtype=torch.long)\n",
    "\n",
    "            label_indices = [char_to_num_dict.get(char, CTC_BLANK_INDEX) for char in alignments_list] # Use blank for OOV\n",
    "            label_tensor = torch.tensor(label_indices, dtype=torch.long)\n",
    "\n",
    "            # --- Augmentation (Applied AFTER ensuring correct shape and normalization) ---\n",
    "            if self.augment and torch.rand(1).item() > 0.5:\n",
    "                frames_tensor = torch.flip(frames_tensor, dims=[2]) # Flip width dimension\n",
    "\n",
    "            return frames_tensor, label_tensor\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found {npy_path}. Skipping.\")\n",
    "            return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                   torch.tensor([], dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item {idx} ({npy_path}): {e}\")\n",
    "            return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                   torch.tensor([], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1976c66-cadc-42de-aadb-631fc8504b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Collate Function ---\n",
    "def collate_fn(batch):\n",
    "    # Filter out samples where loading failed (indicated by empty label tensor)\n",
    "    batch = [(frames, labels) for frames, labels in batch if labels.numel() > 0]\n",
    "    if not batch:\n",
    "        # Return empty tensors if the whole batch failed\n",
    "        return torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "\n",
    "    # Separate frames and labels\n",
    "    frames_list, labels_list = zip(*batch)\n",
    "\n",
    "    # Stack frames (already same size T, H, W, C)\n",
    "    frames_batch = torch.stack(frames_list, dim=0)\n",
    "\n",
    "    # Pad labels\n",
    "    label_lengths = torch.tensor([len(lbl) for lbl in labels_list], dtype=torch.long)\n",
    "    labels_padded = pad_sequence(labels_list, batch_first=True, padding_value=LABEL_PADDING_VALUE)\n",
    "\n",
    "    # Input lengths for CTC (fixed frame count here)\n",
    "    input_lengths = torch.full(size=(len(batch),), fill_value=FRAME_COUNT, dtype=torch.long)\n",
    "\n",
    "    return frames_batch, labels_padded, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5965b310-b0b2-4ce7-b6c0-cff303debedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created.\n",
      "Train Batch Shapes: torch.Size([32, 75, 50, 100, 3]) torch.Size([32, 28]) torch.Size([32]) torch.Size([32])\n",
      "Test Batch Shapes: torch.Size([32, 75, 50, 100, 3]) torch.Size([32, 30]) torch.Size([32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loaders ---\n",
    "BATCH_SIZE_TRAIN = 32 # Adjust based on GPU memory\n",
    "BATCH_SIZE_TEST = 32\n",
    "\n",
    "train_dataset = GRIDDataset(train_files, augment=True)\n",
    "test_dataset = GRIDDataset(test_files, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False,\n",
    "                         collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"DataLoaders created.\")\n",
    "# Optional: Test one batch\n",
    "try:\n",
    "    d_frames, d_labels, d_in_len, d_lbl_len = next(iter(train_loader))\n",
    "    print(\"Train Batch Shapes:\", d_frames.shape, d_labels.shape, d_in_len.shape, d_lbl_len.shape)\n",
    "    d_frames, d_labels, d_in_len, d_lbl_len = next(iter(test_loader))\n",
    "    print(\"Test Batch Shapes:\", d_frames.shape, d_labels.shape, d_in_len.shape, d_lbl_len.shape)\n",
    "except Exception as e:\n",
    "     print(f\"Error fetching batch: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "984a9fb3-c55e-4c55-af41-0d143b559140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch LipNet Model ---\n",
    "class LipNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_p=DROPOUT_P):\n",
    "        super(LipNet, self).__init__()\n",
    "        self.num_classes = num_classes # Should include blank token\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Spatiotemporal Convolutional Layers (STCNN)\n",
    "        self.conv1 = nn.Conv3d(FRAME_CHANNELS, 32, kernel_size=(3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop1 = nn.Dropout3d(dropout_p) # Using Dropout3d for spatial dropout effect\n",
    "\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop2 = nn.Dropout3d(dropout_p)\n",
    "\n",
    "        self.conv3 = nn.Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.bn3 = nn.BatchNorm3d(96)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop3 = nn.Dropout3d(dropout_p)\n",
    "\n",
    "        # Calculate flattened feature size after STCNN and pooling\n",
    "        # Input: (N, C, T, H, W) = (N, 3, 75, 50, 100)\n",
    "        # After conv1+pool1: (N, 32, 75, 12, 25) # H=(50/2)/2=12, W=(100/2)/2=25\n",
    "        # After conv2+pool2: (N, 64, 75, 6, 12)  # H=(12/2)=6, W=(25/2)=12\n",
    "        # After conv3+pool3: (N, 96, 75, 3, 6)   # H=(6/2)=3, W=(12/2)=6\n",
    "        self.rnn_input_size = 96 * 3 * 6 # C_out * H_out * W_out\n",
    "\n",
    "        # Bidirectional GRU Layers\n",
    "        self.gru1 = nn.GRU(self.rnn_input_size, 256, bidirectional=True, batch_first=True)\n",
    "        self.drop_gru1 = nn.Dropout(dropout_p)\n",
    "        self.gru2 = nn.GRU(256 * 2, 256, bidirectional=True, batch_first=True) # Input size is doubled from previous BiGRU\n",
    "        self.drop_gru2 = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(256 * 2, self.num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        # self._initialize_weights()\n",
    "\n",
    "    # def _initialize_weights(self):\n",
    "    #     # Kaiming (He) for Conv, Xavier (Glorot) for GRU kernel, Orthogonal for GRU recurrent\n",
    "    #     for m in self.modules():\n",
    "    #         if isinstance(m, nn.Conv3d):\n",
    "    #             init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    #             if m.bias is not None:\n",
    "    #                 init.constant_(m.bias, 0)\n",
    "    #         elif isinstance(m, nn.BatchNorm3d):\n",
    "    #             init.constant_(m.weight, 1)\n",
    "    #             init.constant_(m.bias, 0)\n",
    "    #         elif isinstance(m, nn.GRU):\n",
    "    #             for name, param in m.named_parameters():\n",
    "    #                 if 'weight_ih' in name: # Input-hidden weights\n",
    "    #                     init.xavier_uniform_(param.data)\n",
    "    #                 elif 'weight_hh' in name: # Hidden-hidden weights\n",
    "    #                     init.orthogonal_(param.data)\n",
    "    #                 elif 'bias' in name: # Biases\n",
    "    #                     init.constant_(param.data, 0)\n",
    "    #                     # Optional: Set forget gate bias to 1 (not directly applicable to GRU like LSTM)\n",
    "    #         elif isinstance(m, nn.Linear):\n",
    "    #              init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') # He for Dense\n",
    "    #              if m.bias is not None:\n",
    "    #                 init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (N, T, H, W, C) from DataLoader\n",
    "        # Permute to (N, C, T, H, W) for Conv3D\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "\n",
    "        # STCNN blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        # Reshape for RNN: (N, C, T, H, W) -> (N, T, C*H*W)\n",
    "        N, C, T, H, W = x.size()\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous() # (N, T, C, H, W)\n",
    "        x = x.view(N, T, -1) # Flatten C, H, W dims\n",
    "\n",
    "        # Bi-GRU layers\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.drop_gru1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.drop_gru2(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        x = self.fc(x) # Output shape: (N, T, num_classes)\n",
    "\n",
    "        # Prepare for CTC Loss: (T, N, C) and apply log_softmax\n",
    "        x = x.permute(1, 0, 2).contiguous() # T, N, C\n",
    "        log_probs = F.log_softmax(x, dim=2)\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d8ccaaf-2b16-44e5-b40f-1a4a1d4b3dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LipNet                                   [75, 32, 28]              --\n",
      "├─Conv3d: 1-1                            [32, 32, 75, 25, 50]      7,232\n",
      "├─BatchNorm3d: 1-2                       [32, 32, 75, 25, 50]      64\n",
      "├─MaxPool3d: 1-3                         [32, 32, 75, 12, 25]      --\n",
      "├─Dropout3d: 1-4                         [32, 32, 75, 12, 25]      --\n",
      "├─Conv3d: 1-5                            [32, 64, 75, 12, 25]      153,664\n",
      "├─BatchNorm3d: 1-6                       [32, 64, 75, 12, 25]      128\n",
      "├─MaxPool3d: 1-7                         [32, 64, 75, 6, 12]       --\n",
      "├─Dropout3d: 1-8                         [32, 64, 75, 6, 12]       --\n",
      "├─Conv3d: 1-9                            [32, 96, 75, 6, 12]       165,984\n",
      "├─BatchNorm3d: 1-10                      [32, 96, 75, 6, 12]       192\n",
      "├─MaxPool3d: 1-11                        [32, 96, 75, 3, 6]        --\n",
      "├─Dropout3d: 1-12                        [32, 96, 75, 3, 6]        --\n",
      "├─GRU: 1-13                              [32, 75, 512]             3,050,496\n",
      "├─Dropout: 1-14                          [32, 75, 512]             --\n",
      "├─GRU: 1-15                              [32, 75, 512]             1,182,720\n",
      "├─Dropout: 1-16                          [32, 75, 512]             --\n",
      "├─Linear: 1-17                           [32, 75, 28]              14,364\n",
      "==========================================================================================\n",
      "Total params: 4,574,844\n",
      "Trainable params: 4,574,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 171.18\n",
      "==========================================================================================\n",
      "Input size (MB): 144.00\n",
      "Forward/backward pass size (MB): 2558.90\n",
      "Params size (MB): 18.30\n",
      "Estimated Total Size (MB): 2721.20\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = LipNet(num_classes=VOCAB_SIZE + 1).to(DEVICE) # +1 for blank\n",
    "\n",
    "# --- Optional: Print Model Summary (requires torchinfo) ---\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "    # Input shape: (N, T, H, W, C)\n",
    "    print(summary(model, input_size=(BATCH_SIZE_TRAIN, FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS)))\n",
    "except ImportError:\n",
    "    print(\"torchinfo not installed. Skipping model summary.\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "884339bd-d7d2-4b0f-9a4f-7a1e8eaad87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loss Function ---\n",
    "# reduction='mean' averages loss over the batch\n",
    "# zero_infinity=True helps prevent NaN/inf gradients if loss explodes\n",
    "ctc_loss = nn.CTCLoss(blank=CTC_BLANK_INDEX, reduction='mean', zero_infinity=True)\n",
    "\n",
    "# --- Optimizer ---\n",
    "LEARNING_RATE = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "723e00e2-f449-4abb-b93d-a296fe8da20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Checkpoint Path ---\n",
    "checkpoint_dir = 'models_pytorch'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'lipnet_checkpoint.pth')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c134986-8e30-41cb-a21d-11974017a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Greedy CTC Decoder (for ProduceExample) ---\n",
    "def greedy_decoder(log_probs, input_lengths):\n",
    "    decoded_sequences = []\n",
    "    # log_probs shape: (T, N, C)\n",
    "    # input_lengths shape: (N,)\n",
    "    for i in range(log_probs.size(1)): # Iterate through batch\n",
    "        # Get relevant log_probs for this sample based on its input length\n",
    "        sample_log_probs = log_probs[:input_lengths[i], i, :]\n",
    "        # Get the best class index at each time step\n",
    "        best_path = torch.argmax(sample_log_probs, dim=1)\n",
    "        # Collapse repeated labels and remove blanks\n",
    "        decoded = []\n",
    "        last_char = -1\n",
    "        for char_idx in best_path:\n",
    "            idx = char_idx.item()\n",
    "            if idx != last_char and idx != CTC_BLANK_INDEX:\n",
    "                decoded.append(idx)\n",
    "            if idx != CTC_BLANK_INDEX: # Update last_char only if it's not blank\n",
    "                last_char = idx\n",
    "        decoded_sequences.append(decoded)\n",
    "    return decoded_sequences\n",
    "\n",
    "# --- ProduceExample Function (Replaces Keras Callback) ---\n",
    "def produce_example(model, dataset_loader, num_to_char_map):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Get one batch from the provided loader (e.g., test_loader.take(1))\n",
    "            frames_batch, labels_batch, input_lengths, label_lengths = next(iter(dataset_loader))\n",
    "            frames_batch = frames_batch.to(DEVICE)\n",
    "            # No need to move labels/lengths to GPU for this function\n",
    "\n",
    "            log_probs = model(frames_batch) # Shape (T, N, C)\n",
    "            # Move log_probs back to CPU for decoding if necessary, ensure input_lengths is CPU tensor\n",
    "            log_probs_cpu = log_probs.cpu()\n",
    "            input_lengths_cpu = input_lengths.cpu()\n",
    "\n",
    "            # Greedy decoding\n",
    "            decoded_indices_list = greedy_decoder(log_probs_cpu, input_lengths_cpu)\n",
    "\n",
    "            print(\"\\n--- Example Predictions ---\")\n",
    "            N_EXAMPLES_TO_SHOW = min(2, frames_batch.size(0))\n",
    "            for i in range(N_EXAMPLES_TO_SHOW):\n",
    "                # Original Label\n",
    "                original_indices = labels_batch[i][:label_lengths[i]].tolist()\n",
    "                original_text = \"\".join([num_to_char_map.get(idx, '?') for idx in original_indices])\n",
    "                print(f'Original:     {original_text}')\n",
    "\n",
    "                # Prediction\n",
    "                prediction_indices = decoded_indices_list[i]\n",
    "                print(f'Filtered Idx: {prediction_indices}') # For debugging\n",
    "                prediction_text = \"\".join([num_to_char_map.get(idx, '?') for idx in prediction_indices])\n",
    "                print(f'Prediction:   {prediction_text}')\n",
    "                print('-'*50)\n",
    "            print(\"--- End Examples ---\\n\")\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"Warning: Could not get a batch from the dataset loader for ProduceExample.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ProduceExample: {e}\")\n",
    "    model.train() # Set model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1edb1f6-c6be-4826-a7f8-2f2cc37043e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from models_pytorch/lipnet_checkpoint.pth\n",
      "Resuming training from epoch 58, Best loss: 2.6130\n",
      "\n",
      "Starting Training...\n",
      "Training batches per epoch: 25\n",
      "Validation batches per epoch: 7\n",
      "\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59 Train: 100%|██████████| 25/25 [04:35<00:00, 11.02s/it, loss=2.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Training Loss: 2.6120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59 Val: 100%|██████████| 7/7 [00:27<00:00,  3.99s/it, val_loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Validation Loss: 2.6470\n",
      "Validation loss (2.6470) did not improve from 2.6130\n",
      "\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60 Train: 100%|██████████| 25/25 [05:00<00:00, 12.04s/it, loss=2.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Training Loss: 2.6077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60 Val: 100%|██████████| 7/7 [00:29<00:00,  4.16s/it, val_loss=2.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Validation Loss: 2.6764\n",
      "Validation loss (2.6764) did not improve from 2.6130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Predictions ---\n",
      "Original:     place green with r seven again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "Original:     bin green with u nine again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "--- End Examples ---\n",
      "\n",
      "\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61 Train: 100%|██████████| 25/25 [05:38<00:00, 13.53s/it, loss=2.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Training Loss: 2.6092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61 Val: 100%|██████████| 7/7 [00:23<00:00,  3.37s/it, val_loss=2.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Validation Loss: 2.6759\n",
      "Validation loss (2.6759) did not improve from 2.6130\n",
      "\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62 Train: 100%|██████████| 25/25 [04:15<00:00, 10.23s/it, loss=2.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Training Loss: 2.6031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62 Val: 100%|██████████| 7/7 [00:23<00:00,  3.41s/it, val_loss=2.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Validation Loss: 2.6398\n",
      "Validation loss (2.6398) did not improve from 2.6130\n",
      "\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63 Train: 100%|██████████| 25/25 [04:16<00:00, 10.26s/it, loss=2.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Training Loss: 2.5961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63 Val: 100%|██████████| 7/7 [00:24<00:00,  3.46s/it, val_loss=2.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Validation Loss: 2.6265\n",
      "Validation loss (2.6265) did not improve from 2.6130\n",
      "\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64 Train: 100%|██████████| 25/25 [04:25<00:00, 10.60s/it, loss=2.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Training Loss: 2.5940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64 Val: 100%|██████████| 7/7 [00:24<00:00,  3.44s/it, val_loss=2.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Validation Loss: 2.5951\n",
      "Validation loss improved from 2.6130 to 2.5951, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65 Train: 100%|██████████| 25/25 [04:44<00:00, 11.38s/it, loss=2.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Training Loss: 2.5931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65 Val: 100%|██████████| 7/7 [00:31<00:00,  4.48s/it, val_loss=2.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Validation Loss: 2.6122\n",
      "Validation loss (2.6122) did not improve from 2.5951\n",
      "\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66 Train:  72%|███████▏  | 18/25 [03:56<01:36, 13.78s/it, loss=2.58]"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "EPOCHS = 100 # As in TF code\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Optional: Load checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['loss']\n",
    "        print(f\"Resuming training from epoch {start_epoch}, Best loss: {best_val_loss:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
    "        start_epoch = 0\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "num_train_batches = len(train_loader)\n",
    "num_test_batches = len(test_loader)\n",
    "print(f\"Training batches per epoch: {num_train_batches}\")\n",
    "print(f\"Validation batches per epoch: {num_test_batches}\")\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_pbar = tqdm(enumerate(train_loader), total=num_train_batches, desc=f\"Epoch {epoch+1} Train\")\n",
    "\n",
    "    for i, batch_data in train_pbar:\n",
    "        frames_batch, labels_padded, input_lengths, label_lengths = batch_data\n",
    "\n",
    "        # Move data to device\n",
    "        frames_batch = frames_batch.to(DEVICE)\n",
    "        labels_padded = labels_padded.to(DEVICE)\n",
    "        input_lengths = input_lengths.to(DEVICE) # Though lengths often stay CPU for CTC\n",
    "        label_lengths = label_lengths.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass -> Logits (T, N, C)\n",
    "        log_probs = model(frames_batch)\n",
    "\n",
    "        # Ensure input lengths don't exceed T\n",
    "        T = log_probs.size(0)\n",
    "        input_lengths = torch.clamp(input_lengths, max=T)\n",
    "\n",
    "        # Calculate CTC Loss\n",
    "        # Ensure labels_padded, input_lengths, label_lengths are on CPU if required by CTCLoss implementation\n",
    "        loss = ctc_loss(log_probs, labels_padded.cpu(), input_lengths.cpu(), label_lengths.cpu())\n",
    "\n",
    "        # Check for NaN/inf loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "             print(f\"Warning: NaN or Inf loss detected at batch {i}. Skipping batch.\")\n",
    "             continue # Skip gradient update for this batch\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_pbar.set_postfix({'loss': running_loss / (i + 1)})\n",
    "\n",
    "    avg_train_loss = running_loss / num_train_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_pbar = tqdm(enumerate(test_loader), total=num_test_batches, desc=f\"Epoch {epoch+1} Val\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in val_pbar:\n",
    "            frames_batch, labels_padded, input_lengths, label_lengths = batch_data\n",
    "            frames_batch = frames_batch.to(DEVICE)\n",
    "            labels_padded = labels_padded.to(DEVICE)\n",
    "            input_lengths = input_lengths.to(DEVICE)\n",
    "            label_lengths = label_lengths.to(DEVICE)\n",
    "\n",
    "            log_probs = model(frames_batch)\n",
    "            T = log_probs.size(0)\n",
    "            input_lengths = torch.clamp(input_lengths, max=T)\n",
    "\n",
    "            loss = ctc_loss(log_probs, labels_padded.cpu(), input_lengths.cpu(), label_lengths.cpu())\n",
    "\n",
    "            if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                running_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'val_loss': running_val_loss / (i + 1)})\n",
    "\n",
    "    avg_val_loss = running_val_loss / num_test_batches\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # --- Save Checkpoint ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}, saving model to {checkpoint_path}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_val_loss,\n",
    "        }, checkpoint_path)\n",
    "        best_val_loss = avg_val_loss\n",
    "    else:\n",
    "        print(f\"Validation loss ({avg_val_loss:.4f}) did not improve from {best_val_loss:.4f}\")\n",
    "\n",
    "    # --- Show Examples (e.g., every 5 epochs) ---\n",
    "    if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:\n",
    "         produce_example(model, test_loader, num_to_char_dict)\n",
    "\n",
    "print(\"\\nTraining Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "033fbf86-b8f4-496e-a34d-d2efc7ebd1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Final Test Examples ---\n",
      "Loading best weights from models_pytorch/lipnet_checkpoint.pth for final test...\n",
      "Best weights loaded successfully.\n",
      "\n",
      "--- Example Predictions ---\n",
      "Original:     place green with r seven again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "Original:     bin green with u nine again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "--- End Examples ---\n",
      "\n",
      "--- Final Testing Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Plotting Training/Validation Loss ---\n",
    "plt.figure(figsize=(8, 3))\n",
    "# plt.plot(range(1, (EPOCHS - start_epoch) + 1), train_losses, color='red', label=\"Training Loss\")\n",
    "# plt.plot(range(1, (EPOCHS - start_epoch) + 1), val_losses, color='green', label=\"Validation Loss\")\n",
    "plt.plot(train_losses, color='red', label=\"Training Loss\")\n",
    "plt.plot(val_losses, color='green', label=\"Validation Loss\")\n",
    "plt.title(\"Training/Validation Loss vs Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_curve_pytorch.png\")\n",
    "plt.show()\n",
    "\n",
    "# --- Final Testing Example (similar to TF code) ---\n",
    "print(\"\\n--- Running Final Test Examples ---\")\n",
    "# Load best weights\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading best weights from {checkpoint_path} for final test...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Best weights loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best weights: {e}\")\n",
    "else:\n",
    "    print(\"No checkpoint found for final testing.\")\n",
    "\n",
    "produce_example(model, test_loader, num_to_char_dict) # Show examples with best model\n",
    "print(\"--- Final Testing Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ade41079-8532-4bcf-a517-606be35085cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking .npy files in ./processed_mouth_data/...\n",
      "Found 1000 files. Checking shapes...\n",
      "PROBLEM: ./processed_mouth_data/s1/lrae3s_mouth.npy has shape (74, 50, 100, 3)\n",
      "PROBLEM: ./processed_mouth_data/s1/sbbbzp_mouth.npy has shape (74, 50, 100, 3)\n",
      "PROBLEM: ./processed_mouth_data/s1/srbb4n_mouth.npy has shape (74, 50, 100, 3)\n",
      "PROBLEM: ./processed_mouth_data/s1/srwi5a_mouth.npy has shape (74, 50, 100, 3)\n",
      "PROBLEM: ./processed_mouth_data/s1/swao7a_mouth.npy has shape (74, 50, 100, 3)\n",
      "\n",
      "Found 5 files with potential shape issues.\n",
      "Problematic files removed. Please re-run preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# BASE_PROCESSED_PATH = './processed_mouth_data/'\n",
    "# FRAME_HEIGHT = 50\n",
    "# FRAME_WIDTH = 100\n",
    "# FRAME_CHANNELS = 3\n",
    "# TARGET_SHAPE_LAST_DIMS = (75, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS)\n",
    "\n",
    "# print(f\"Checking .npy files in {BASE_PROCESSED_PATH}...\")\n",
    "# problem_files = []\n",
    "# all_files = glob.glob(os.path.join(BASE_PROCESSED_PATH, 's*', '*.npy'))\n",
    "\n",
    "# if not all_files:\n",
    "#     print(\"No .npy files found. Make sure BASE_PROCESSED_PATH is correct.\")\n",
    "# else:\n",
    "#     print(f\"Found {len(all_files)} files. Checking shapes...\")\n",
    "#     for npy_file in all_files:\n",
    "#         try:\n",
    "#             data = np.load(npy_file)\n",
    "#             if data.ndim != 4 or data.shape[:] != TARGET_SHAPE_LAST_DIMS:\n",
    "#                 print(f\"PROBLEM: {npy_file} has shape {data.shape}\")\n",
    "#                 problem_files.append(npy_file)\n",
    "#         except Exception as e:\n",
    "#             print(f\"ERROR reading {npy_file}: {e}\")\n",
    "#             problem_files.append(npy_file)\n",
    "\n",
    "#     if not problem_files:\n",
    "#         print(\"All checked .npy files seem to have correct H, W, C dimensions.\")\n",
    "#     else:\n",
    "#         print(f\"\\nFound {len(problem_files)} files with potential shape issues.\")\n",
    "#         # Optionally, you could delete or reprocess these problem files\n",
    "#         for f_path in problem_files:\n",
    "#             os.remove(f_path)\n",
    "#         print(\"Problematic files removed. Please re-run preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c6b21-846a-4c2b-b2f1-7d15f749b46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
