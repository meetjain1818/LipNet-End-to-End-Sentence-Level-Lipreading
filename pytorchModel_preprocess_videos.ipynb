{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd86eb1-901a-457d-af93-7ea0d08cb4cb",
   "metadata": {},
   "source": [
    "# Notebook to convert the videos into npy files which will be used for training the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749d5270-043f-4e66-8a5a-c7e08ca84fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional\n",
    "import imageio\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc1f6786-cd26-49c4-8dd0-f419184505b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL_SPEAKER_IDS = [f's{i}_processed' for i in range(1, 5) if i != 21]\n",
    "ALL_SPEAKER_IDS = [\"s31_processed\", \"s32_processed\", \"s33_processed\"]\n",
    "BASE_PROCESSED_PATH = './GRIDCorpus/processed_mouth_data/'\n",
    "FRAME_COUNT = 75\n",
    "FRAME_HEIGHT = 50\n",
    "FRAME_WIDTH = 100\n",
    "FRAME_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "655fa55a-2bd4-40cc-8514-d635f498960c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s31_processed', 's32_processed', 's33_processed']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_SPEAKER_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10f6d26-5145-430e-92fc-9285987bc22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting video preprocessing...\n",
      "Processing speaker: s31_processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speaker s31_processed:  94%|█████████▍| 945/1000 [19:54<01:09,  1.27s/video][mpeg1video @ 0x571bcc0] ac-tex damaged at 22 17\n",
      "[mpeg1video @ 0x571bcc0] Warning MVs not available\n",
      "Speaker s31_processed: 100%|██████████| 1000/1000 [21:03<00:00,  1.26s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing speaker: s32_processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speaker s32_processed: 100%|██████████| 1000/1000 [20:16<00:00,  1.22s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing speaker: s33_processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speaker s33_processed: 100%|██████████| 1000/1000 [20:04<00:00,  1.20s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing finished. Processed: 3000, Skipped (already exists): 0\n",
      "Preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Mouth Extraction (Mostly unchanged, ensure return type is numpy) ---\n",
    "try:\n",
    "    DLIB_LANDMARK_PREDICTOR = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    if not os.path.exists(DLIB_LANDMARK_PREDICTOR):\n",
    "        # Add download/unzip logic here if needed, e.g., using requests/bz2\n",
    "        print(f\"Error: dlib landmark predictor '{DLIB_LANDMARK_PREDICTOR}' not found.\")\n",
    "        print(\"Please download it from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "        exit()\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(DLIB_LANDMARK_PREDICTOR)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing dlib: {e}\")\n",
    "    print(\"Make sure dlib is installed correctly and the predictor file exists.\")\n",
    "    exit()\n",
    "\n",
    "def extract_mouth_region(frame: np.ndarray) -> Optional[np.ndarray]:\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    face = faces[0]\n",
    "    landmarks = predictor(gray, face)\n",
    "    points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)])\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(points)\n",
    "    # Adjust margins carefully - TF code used slightly different logic\n",
    "    # Let's try to match the TF code's effective crop:\n",
    "    y_start = max(y + 15 - 30, 0) # y + 15 was start, margin was 30\n",
    "    y_end = y + 15 + h + 30\n",
    "    x_start = max(x + 15 - 30, 0) # x + 15 was start, margin was 30\n",
    "    x_end = x + 15 + w + 30\n",
    "\n",
    "    # Ensure coordinates are within frame bounds\n",
    "    y_start = max(0, y_start)\n",
    "    y_end = min(frame.shape[0], y_end)\n",
    "    x_start = max(0, x_start)\n",
    "    x_end = min(frame.shape[1], x_end)\n",
    "\n",
    "    cropped = frame[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    if cropped.size == 0: # Handle empty crop\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "    try:\n",
    "        cropped = cv2.resize(cropped, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "        return cropped\n",
    "    except cv2.error as e:\n",
    "        print(f\"Warning: cv2.resize error ({e}). Returning zero frame.\")\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "def process_and_save_video(video_path: str, output_dir: str) -> None:\n",
    "    \"\"\"Processes a single video, extracts mouth regions, and saves as .npy\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening video file: {video_path}\")\n",
    "            return None\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            mouth_frame = extract_mouth_region(frame)\n",
    "            if mouth_frame is not None:\n",
    "                frames.append(mouth_frame)\n",
    "            else:\n",
    "                # If detection fails, append a zero frame\n",
    "                frames.append(np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8))\n",
    "        cap.release()\n",
    "\n",
    "        if not frames:\n",
    "            print(f\"Warning: No frames extracted from {video_path}\")\n",
    "            return None\n",
    "\n",
    "        # Ensure video has FRAME_COUNT frames (pad/truncate if needed)\n",
    "        frames_np = np.array(frames, dtype=np.uint8) # Keep as uint8 for now\n",
    "        current_frame_count = frames_np.shape[0]\n",
    "        if current_frame_count != FRAME_COUNT:\n",
    "             if current_frame_count > FRAME_COUNT:\n",
    "                 frames_np = frames_np[:FRAME_COUNT, ...]\n",
    "             else:\n",
    "                 pad_width = ((0, FRAME_COUNT - current_frame_count), (0, 0), (0, 0), (0, 0))\n",
    "                 frames_np = np.pad(frames_np, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "        video_id = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        output_path = os.path.join(output_dir, f\"{video_id}_mouth.npy\")\n",
    "        np.save(output_path, frames_np)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Function to preprocess all videos (Run only once) ---\n",
    "def preprocess_all_videos(force_reprocess=False):\n",
    "    print(\"Starting video preprocessing...\")\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    for speaker_id in ALL_SPEAKER_IDS:\n",
    "        input_videos_dir = os.path.join('./GRIDCorpus/data/', speaker_id) # Assuming video folder structure\n",
    "        output_preprocessed_dir = os.path.join(BASE_PROCESSED_PATH, speaker_id)\n",
    "        os.makedirs(output_preprocessed_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing speaker: {speaker_id}\")\n",
    "        video_files = glob.glob(os.path.join(input_videos_dir, '*.mpg')) # Assuming .mpg format\n",
    "\n",
    "        if not video_files:\n",
    "             print(f\"  Warning: No .mpg files found in {input_videos_dir}\")\n",
    "             continue\n",
    "\n",
    "        for video_file in tqdm(video_files, desc=f\"Speaker {speaker_id}\", unit=\"video\"):\n",
    "             video_id = os.path.splitext(os.path.basename(video_file))[0]\n",
    "             output_path = os.path.join(output_preprocessed_dir, f\"{video_id}_mouth.npy\")\n",
    "             if not force_reprocess and os.path.exists(output_path):\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "             process_and_save_video(video_file, output_preprocessed_dir)\n",
    "             processed_count += 1\n",
    "\n",
    "    print(f\"\\nPreprocessing finished. Processed: {processed_count}, Skipped (already exists): {skipped_count}\")\n",
    "\n",
    "# --- UNCOMMENT AND RUN THIS ONCE TO PREPROCESS ---\n",
    "preprocess_all_videos(force_reprocess=False)\n",
    "print(\"Preprocessing complete.\")\n",
    "# --- END PREPROCESSING CALL ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3230bba0-5321-45c0-b22f-057e9a86cf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"./GRIDCorpus/processed_mouth_data/s23_processed\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
