{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf1ece22-b52e-440b-b9f5-e702e54eedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional\n",
    "import imageio\n",
    "# import gdown # Removed, assuming data is downloaded\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-dark-palette') # Use a compatible style\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63956a29-bc4d-4178-85eb-cfecf0cf6d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9ff14b-bfef-4f0c-9378-307c9d7902cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_COUNT = 75\n",
    "FRAME_HEIGHT = 50\n",
    "FRAME_WIDTH = 100\n",
    "FRAME_CHANNELS = 3\n",
    "DROPOUT_P = 0.5\n",
    "\n",
    "BASE_PROCESSED_PATH = './processed_mouth_data/'\n",
    "BASE_ALIGN_PATH = './data/alignments/'\n",
    "\n",
    "ALL_SPEAKER_IDS = [f's{i}' for i in range(1, 2) if i != 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72549801-190e-4f2f-b05a-d65f533e208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_SPEAKERS_UNSEEN = ['s1', 's2', 's20', 's22']\n",
    "NUM_TEST_SENTENCES_OVERLAPPED = 200 \n",
    "SPLIT_MODE = 'overlapped' # Choose 'unseen' or 'overlapped'\n",
    "\n",
    "# Normalization constants\n",
    "NORM_MEAN = np.array([0.7136, 0.4906, 0.3283], dtype=np.float32)\n",
    "NORM_STD = np.array([0.1138, 0.1078, 0.0917], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2513c15e-8248-48ef-8868-76f8d5b76333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' '] (size =27)\n",
      "CTC Blank Index: 0\n",
      "Label Padding Value: 0\n"
     ]
    }
   ],
   "source": [
    "# --- Vocabulary ---\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz \"]\n",
    "char_to_num_dict = {char: i + 1 for i, char in enumerate(vocab)} # Start indices from 1\n",
    "num_to_char_dict = {i + 1: char for i, char in enumerate(vocab)}\n",
    "# Add OOV/padding token if needed by your collate logic, but CTC uses blank\n",
    "VOCAB_SIZE = len(vocab)\n",
    "CTC_BLANK_INDEX = 0 # CTC blank is often index 0 by convention in PyTorch\n",
    "LABEL_PADDING_VALUE = CTC_BLANK_INDEX # Use blank index for padding labels\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary is: {vocab} \"\n",
    "    f\"(size ={VOCAB_SIZE})\"\n",
    ")\n",
    "print(f\"CTC Blank Index: {CTC_BLANK_INDEX}\")\n",
    "print(f\"Label Padding Value: {LABEL_PADDING_VALUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf8b369-4c32-4436-b361-58c60ac52760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mouth Extraction (Mostly unchanged, ensure return type is numpy) ---\n",
    "try:\n",
    "    DLIB_LANDMARK_PREDICTOR = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    if not os.path.exists(DLIB_LANDMARK_PREDICTOR):\n",
    "        # Add download/unzip logic here if needed, e.g., using requests/bz2\n",
    "        print(f\"Error: dlib landmark predictor '{DLIB_LANDMARK_PREDICTOR}' not found.\")\n",
    "        print(\"Please download it from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "        exit()\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(DLIB_LANDMARK_PREDICTOR)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing dlib: {e}\")\n",
    "    print(\"Make sure dlib is installed correctly and the predictor file exists.\")\n",
    "    exit()\n",
    "\n",
    "def extract_mouth_region(frame: np.ndarray) -> Optional[np.ndarray]:\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    face = faces[0]\n",
    "    landmarks = predictor(gray, face)\n",
    "    points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)])\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(points)\n",
    "    # Adjust margins carefully - TF code used slightly different logic\n",
    "    # Let's try to match the TF code's effective crop:\n",
    "    y_start = max(y + 15 - 30, 0) # y + 15 was start, margin was 30\n",
    "    y_end = y + 15 + h + 30\n",
    "    x_start = max(x + 15 - 30, 0) # x + 15 was start, margin was 30\n",
    "    x_end = x + 15 + w + 30\n",
    "\n",
    "    # Ensure coordinates are within frame bounds\n",
    "    y_start = max(0, y_start)\n",
    "    y_end = min(frame.shape[0], y_end)\n",
    "    x_start = max(0, x_start)\n",
    "    x_end = min(frame.shape[1], x_end)\n",
    "\n",
    "    cropped = frame[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    if cropped.size == 0: # Handle empty crop\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "    try:\n",
    "        cropped = cv2.resize(cropped, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "        return cropped\n",
    "    except cv2.error as e:\n",
    "        print(f\"Warning: cv2.resize error ({e}). Returning zero frame.\")\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "def process_and_save_video(video_path: str, output_dir: str) -> None:\n",
    "    \"\"\"Processes a single video, extracts mouth regions, and saves as .npy\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening video file: {video_path}\")\n",
    "            return None\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            mouth_frame = extract_mouth_region(frame)\n",
    "            if mouth_frame is not None:\n",
    "                frames.append(mouth_frame)\n",
    "            else:\n",
    "                # If detection fails, append a zero frame\n",
    "                frames.append(np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8))\n",
    "        cap.release()\n",
    "\n",
    "        if not frames:\n",
    "            print(f\"Warning: No frames extracted from {video_path}\")\n",
    "            return None\n",
    "\n",
    "        # Ensure video has FRAME_COUNT frames (pad/truncate if needed)\n",
    "        frames_np = np.array(frames, dtype=np.uint8) # Keep as uint8 for now\n",
    "        current_frame_count = frames_np.shape[0]\n",
    "        if current_frame_count != FRAME_COUNT:\n",
    "             if current_frame_count > FRAME_COUNT:\n",
    "                 frames_np = frames_np[:FRAME_COUNT, ...]\n",
    "             else:\n",
    "                 pad_width = ((0, FRAME_COUNT - current_frame_count), (0, 0), (0, 0), (0, 0))\n",
    "                 frames_np = np.pad(frames_np, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "        video_id = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        output_path = os.path.join(output_dir, f\"{video_id}_mouth.npy\")\n",
    "        np.save(output_path, frames_np)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Function to preprocess all videos (Run only once) ---\n",
    "def preprocess_all_videos(force_reprocess=False):\n",
    "    print(\"Starting video preprocessing...\")\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    for speaker_id in ALL_SPEAKER_IDS:\n",
    "        input_videos_dir = os.path.join('./data/', speaker_id, 'video') # Assuming video folder structure\n",
    "        output_preprocessed_dir = os.path.join(BASE_PROCESSED_PATH, speaker_id)\n",
    "        os.makedirs(output_preprocessed_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing speaker: {speaker_id}\")\n",
    "        video_files = glob.glob(os.path.join(input_videos_dir, '*.mpg')) # Assuming .mpg format\n",
    "\n",
    "        if not video_files:\n",
    "             print(f\"  Warning: No .mpg files found in {input_videos_dir}\")\n",
    "             continue\n",
    "\n",
    "        for video_file in tqdm(video_files, desc=f\"Speaker {speaker_id}\", unit=\"video\"):\n",
    "             video_id = os.path.splitext(os.path.basename(video_file))[0]\n",
    "             output_path = os.path.join(output_preprocessed_dir, f\"{video_id}_mouth.npy\")\n",
    "             if not force_reprocess and os.path.exists(output_path):\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "             process_and_save_video(video_file, output_preprocessed_dir)\n",
    "             processed_count += 1\n",
    "\n",
    "    print(f\"\\nPreprocessing finished. Processed: {processed_count}, Skipped (already exists): {skipped_count}\")\n",
    "\n",
    "# --- UNCOMMENT AND RUN THIS ONCE TO PREPROCESS ---\n",
    "# preprocess_all_videos(force_reprocess=False)\n",
    "# print(\"Preprocessing complete. You can now comment out the preprocess_all_videos call.\")\n",
    "# --- END PREPROCESSING CALL ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3473b7c4-e4a9-43b8-9ef0-40424b9e6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GIF Creation (Unchanged) ---\n",
    "def create_gif_from_npy(npy_path: str, gif_path: str, duration: float = 0.04) -> None:\n",
    "    try:\n",
    "        frames = np.load(npy_path) # Should be uint8\n",
    "        imageio.mimsave(gif_path, frames, duration=duration)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Npy file not found at {npy_path}, cannot create GIF.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating GIF from {npy_path}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8424bde4-0652-4476-a445-3757e6e72fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Alignment Loading (Unchanged, returns list of chars) ---\n",
    "def load_alignments(align_file: str) -> Optional[List[str]]:\n",
    "    alignments_chars = []\n",
    "    try:\n",
    "        with open(align_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                _, _, token = parts\n",
    "                if token != 'sil':\n",
    "                    token_characters = list(token.lower() + ' ')\n",
    "                    alignments_chars.extend(token_characters)\n",
    "        # Remove the trailing space from the last word\n",
    "        return alignments_chars[:-1] if alignments_chars else []\n",
    "    except FileNotFoundError:\n",
    "         print(f\"Warning: Alignment file not found: {align_file}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading alignments {align_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14035da-9807-462e-9066-c4aca1bb0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'a', 'y', ' ', 'r', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'z', ' ', 'f', 'o', 'u', 'r', ' ', 'p', 'l', 'e', 'a', 's', 'e']\n"
     ]
    }
   ],
   "source": [
    "example_alignment = load_alignments(\"./data/alignments/s1/lrwz4p.align\")\n",
    "print(example_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338b18fd-fec0-4a19-99ce-7fdbd8eb4e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using overlapped speakers split. 200 test sentences per speaker.\n",
      "Total files found: 995\n",
      "Training files: 795\n",
      "Test files: 200\n"
     ]
    }
   ],
   "source": [
    "# --- File Listing and Splitting ---\n",
    "all_npy_files = []\n",
    "for speaker_id in ALL_SPEAKER_IDS:\n",
    "    speaker_path = os.path.join(BASE_PROCESSED_PATH, speaker_id, '*.npy')\n",
    "    files = glob.glob(speaker_path)\n",
    "    if not files:\n",
    "        print(f\"Warning: No .npy files found for speaker {speaker_id} in {os.path.join(BASE_PROCESSED_PATH, speaker_id)}\")\n",
    "    all_npy_files.extend(files)\n",
    "\n",
    "if not all_npy_files:\n",
    "    raise FileNotFoundError(f\"No .npy files found in {BASE_PROCESSED_PATH} for speakers {ALL_SPEAKER_IDS}. Did preprocessing run?\")\n",
    "\n",
    "np.random.shuffle(all_npy_files)\n",
    "train_files, test_files = [], []\n",
    "\n",
    "if SPLIT_MODE == 'unseen':\n",
    "    print(f\"Using unseen speakers split. Test speakers: {TEST_SPEAKERS_UNSEEN}\")\n",
    "    for f in all_npy_files:\n",
    "        speaker_id = os.path.basename(os.path.dirname(f))\n",
    "        if speaker_id in TEST_SPEAKERS_UNSEEN:\n",
    "            test_files.append(f)\n",
    "        else:\n",
    "            train_files.append(f)\n",
    "elif SPLIT_MODE == 'overlapped':\n",
    "    print(f\"Using overlapped speakers split. {NUM_TEST_SENTENCES_OVERLAPPED} test sentences per speaker.\")\n",
    "    files_by_speaker = {}\n",
    "    for f in all_npy_files:\n",
    "        speaker_id = os.path.basename(os.path.dirname(f))\n",
    "        if speaker_id not in files_by_speaker:\n",
    "            files_by_speaker[speaker_id] = []\n",
    "        files_by_speaker[speaker_id].append(f)\n",
    "\n",
    "    for speaker_id, files in files_by_speaker.items():\n",
    "        np.random.shuffle(files)\n",
    "        test_count = min(NUM_TEST_SENTENCES_OVERLAPPED, len(files))\n",
    "        test_files.extend(files[:test_count])\n",
    "        train_files.extend(files[test_count:])\n",
    "else:\n",
    "    raise ValueError(\"Invalid SPLIT_MODE. Choose 'unseen' or 'overlapped'.\")\n",
    "\n",
    "print(f\"Total files found: {len(all_npy_files)}\")\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")\n",
    "\n",
    "if not train_files or not test_files:\n",
    "    raise ValueError(\"Training or test set is empty. Check file paths and splitting logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "834ccdb7-5ff3-44e1-a7d0-b3411f81f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch Dataset ---\n",
    "class GRIDDataset(Dataset):\n",
    "    def __init__(self, file_paths: List[str], augment: bool = False):\n",
    "        self.file_paths = file_paths\n",
    "        self.augment = augment\n",
    "        self.target_frame_count = FRAME_COUNT # Define target length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        npy_path = self.file_paths[idx]\n",
    "        try:\n",
    "            # --- Load Frames (Keep as uint8 for now) ---\n",
    "            # This part loads the preprocessed mouth crops saved earlier\n",
    "            frames_uint8 = np.load(npy_path)\n",
    "\n",
    "            if frames_uint8.ndim != 4 or frames_uint8.shape[1:] != (FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS):\n",
    "                 print(f\"Warning: Unexpected shape {frames_uint8.shape} for {npy_path}. Skipping.\")\n",
    "                 # Return dummy data that collate_fn can handle/filter\n",
    "                 return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                        torch.tensor([], dtype=torch.long)\n",
    "\n",
    "            # --- Ensure Correct Frame Count ---\n",
    "            current_frame_count = frames_uint8.shape[0]\n",
    "            if current_frame_count != self.target_frame_count:\n",
    "                if current_frame_count > self.target_frame_count:\n",
    "                    # Truncate\n",
    "                    frames_uint8 = frames_uint8[:self.target_frame_count, ...]\n",
    "                else:\n",
    "                    # Pad with zeros\n",
    "                    pad_width = ((0, self.target_frame_count - current_frame_count), (0, 0), (0, 0), (0, 0))\n",
    "                    # Ensure padding uses the correct dtype (uint8) before conversion\n",
    "                    frames_uint8 = np.pad(frames_uint8, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "            # --- Convert to Float, Normalize, Convert BGR->RGB ---\n",
    "            frames_float = frames_uint8.astype(np.float32) / 255.0\n",
    "            frames_rgb = frames_float[..., ::-1] # BGR to RGB\n",
    "            frames_normalized = (frames_rgb - NORM_MEAN) / NORM_STD\n",
    "            frames_tensor = torch.tensor(frames_normalized, dtype=torch.float32)\n",
    "            # Shape should now be guaranteed [75, 50, 100, 3]\n",
    "\n",
    "            # --- Get Alignments ---\n",
    "            # (Rest of your alignment loading logic remains the same)\n",
    "            parts = npy_path.split(os.path.sep)\n",
    "            speaker_id = parts[-2]\n",
    "            base_name = os.path.splitext(parts[-1])[0]\n",
    "            if base_name.endswith('_mouth'):\n",
    "                base_name = base_name[:-6]\n",
    "            align_file = os.path.join(BASE_ALIGN_PATH, speaker_id, f'{base_name}.align')\n",
    "            alignments_list = load_alignments(align_file)\n",
    "\n",
    "            if alignments_list is None:\n",
    "                print(f\"Skipping {npy_path} due to missing alignment.\")\n",
    "                return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                       torch.tensor([], dtype=torch.long)\n",
    "\n",
    "            label_indices = [char_to_num_dict.get(char, CTC_BLANK_INDEX) for char in alignments_list] # Use blank for OOV\n",
    "            label_tensor = torch.tensor(label_indices, dtype=torch.long)\n",
    "\n",
    "            # --- Augmentation (Applied AFTER ensuring correct shape and normalization) ---\n",
    "            if self.augment and torch.rand(1).item() > 0.5:\n",
    "                frames_tensor = torch.flip(frames_tensor, dims=[2]) # Flip width dimension\n",
    "\n",
    "            return frames_tensor, label_tensor\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found {npy_path}. Skipping.\")\n",
    "            return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                   torch.tensor([], dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item {idx} ({npy_path}): {e}\")\n",
    "            return torch.zeros((FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS), dtype=torch.float32), \\\n",
    "                   torch.tensor([], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1976c66-cadc-42de-aadb-631fc8504b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Collate Function ---\n",
    "def collate_fn(batch):\n",
    "    # Filter out samples where loading failed (indicated by empty label tensor)\n",
    "    batch = [(frames, labels) for frames, labels in batch if labels.numel() > 0]\n",
    "    if not batch:\n",
    "        # Return empty tensors if the whole batch failed\n",
    "        return torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "\n",
    "    # Separate frames and labels\n",
    "    frames_list, labels_list = zip(*batch)\n",
    "\n",
    "    # Stack frames (already same size T, H, W, C)\n",
    "    frames_batch = torch.stack(frames_list, dim=0)\n",
    "\n",
    "    # Pad labels\n",
    "    label_lengths = torch.tensor([len(lbl) for lbl in labels_list], dtype=torch.long)\n",
    "    labels_padded = pad_sequence(labels_list, batch_first=True, padding_value=LABEL_PADDING_VALUE)\n",
    "\n",
    "    # Input lengths for CTC (fixed frame count here)\n",
    "    input_lengths = torch.full(size=(len(batch),), fill_value=FRAME_COUNT, dtype=torch.long)\n",
    "\n",
    "    return frames_batch, labels_padded, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5965b310-b0b2-4ce7-b6c0-cff303debedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created.\n",
      "Train Batch Shapes: torch.Size([32, 75, 50, 100, 3]) torch.Size([32, 28]) torch.Size([32]) torch.Size([32])\n",
      "Test Batch Shapes: torch.Size([32, 75, 50, 100, 3]) torch.Size([32, 30]) torch.Size([32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loaders ---\n",
    "BATCH_SIZE_TRAIN = 32 # Adjust based on GPU memory\n",
    "BATCH_SIZE_TEST = 32\n",
    "\n",
    "train_dataset = GRIDDataset(train_files, augment=True)\n",
    "test_dataset = GRIDDataset(test_files, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False,\n",
    "                         collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"DataLoaders created.\")\n",
    "# Optional: Test one batch\n",
    "try:\n",
    "    d_frames, d_labels, d_in_len, d_lbl_len = next(iter(train_loader))\n",
    "    print(\"Train Batch Shapes:\", d_frames.shape, d_labels.shape, d_in_len.shape, d_lbl_len.shape)\n",
    "    d_frames, d_labels, d_in_len, d_lbl_len = next(iter(test_loader))\n",
    "    print(\"Test Batch Shapes:\", d_frames.shape, d_labels.shape, d_in_len.shape, d_lbl_len.shape)\n",
    "except Exception as e:\n",
    "     print(f\"Error fetching batch: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984a9fb3-c55e-4c55-af41-0d143b559140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch LipNet Model ---\n",
    "class LipNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_p=DROPOUT_P):\n",
    "        super(LipNet, self).__init__()\n",
    "        self.num_classes = num_classes # Should include blank token\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Spatiotemporal Convolutional Layers (STCNN)\n",
    "        self.conv1 = nn.Conv3d(FRAME_CHANNELS, 32, kernel_size=(3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop1 = nn.Dropout3d(dropout_p) # Using Dropout3d for spatial dropout effect\n",
    "\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop2 = nn.Dropout3d(dropout_p)\n",
    "\n",
    "        self.conv3 = nn.Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.bn3 = nn.BatchNorm3d(96)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.drop3 = nn.Dropout3d(dropout_p)\n",
    "\n",
    "        # Calculate flattened feature size after STCNN and pooling\n",
    "        # Input: (N, C, T, H, W) = (N, 3, 75, 50, 100)\n",
    "        # After conv1+pool1: (N, 32, 75, 12, 25) # H=(50/2)/2=12, W=(100/2)/2=25\n",
    "        # After conv2+pool2: (N, 64, 75, 6, 12)  # H=(12/2)=6, W=(25/2)=12\n",
    "        # After conv3+pool3: (N, 96, 75, 3, 6)   # H=(6/2)=3, W=(12/2)=6\n",
    "        self.rnn_input_size = 96 * 3 * 6 # C_out * H_out * W_out\n",
    "\n",
    "        # Bidirectional GRU Layers\n",
    "        self.gru1 = nn.GRU(self.rnn_input_size, 256, bidirectional=True, batch_first=True)\n",
    "        self.drop_gru1 = nn.Dropout(dropout_p)\n",
    "        self.gru2 = nn.GRU(256 * 2, 256, bidirectional=True, batch_first=True) # Input size is doubled from previous BiGRU\n",
    "        self.drop_gru2 = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(256 * 2, self.num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Kaiming (He) for Conv, Xavier (Glorot) for GRU kernel, Orthogonal for GRU recurrent\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name: # Input-hidden weights\n",
    "                        init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name: # Hidden-hidden weights\n",
    "                        init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name: # Biases\n",
    "                        init.constant_(param.data, 0)\n",
    "                        # Optional: Set forget gate bias to 1 (not directly applicable to GRU like LSTM)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                 init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') # He for Dense\n",
    "                 if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (N, T, H, W, C) from DataLoader\n",
    "        # Permute to (N, C, T, H, W) for Conv3D\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "\n",
    "        # STCNN blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        # Reshape for RNN: (N, C, T, H, W) -> (N, T, C*H*W)\n",
    "        N, C, T, H, W = x.size()\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous() # (N, T, C, H, W)\n",
    "        x = x.view(N, T, -1) # Flatten C, H, W dims\n",
    "\n",
    "        # Bi-GRU layers\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.drop_gru1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.drop_gru2(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        x = self.fc(x) # Output shape: (N, T, num_classes)\n",
    "\n",
    "        # Prepare for CTC Loss: (T, N, C) and apply log_softmax\n",
    "        x = x.permute(1, 0, 2).contiguous() # T, N, C\n",
    "        log_probs = F.log_softmax(x, dim=2)\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8ccaaf-2b16-44e5-b40f-1a4a1d4b3dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LipNet                                   [75, 32, 28]              --\n",
      "├─Conv3d: 1-1                            [32, 32, 75, 25, 50]      7,232\n",
      "├─BatchNorm3d: 1-2                       [32, 32, 75, 25, 50]      64\n",
      "├─MaxPool3d: 1-3                         [32, 32, 75, 12, 25]      --\n",
      "├─Dropout3d: 1-4                         [32, 32, 75, 12, 25]      --\n",
      "├─Conv3d: 1-5                            [32, 64, 75, 12, 25]      153,664\n",
      "├─BatchNorm3d: 1-6                       [32, 64, 75, 12, 25]      128\n",
      "├─MaxPool3d: 1-7                         [32, 64, 75, 6, 12]       --\n",
      "├─Dropout3d: 1-8                         [32, 64, 75, 6, 12]       --\n",
      "├─Conv3d: 1-9                            [32, 96, 75, 6, 12]       165,984\n",
      "├─BatchNorm3d: 1-10                      [32, 96, 75, 6, 12]       192\n",
      "├─MaxPool3d: 1-11                        [32, 96, 75, 3, 6]        --\n",
      "├─Dropout3d: 1-12                        [32, 96, 75, 3, 6]        --\n",
      "├─GRU: 1-13                              [32, 75, 512]             3,050,496\n",
      "├─Dropout: 1-14                          [32, 75, 512]             --\n",
      "├─GRU: 1-15                              [32, 75, 512]             1,182,720\n",
      "├─Dropout: 1-16                          [32, 75, 512]             --\n",
      "├─Linear: 1-17                           [32, 75, 28]              14,364\n",
      "==========================================================================================\n",
      "Total params: 4,574,844\n",
      "Trainable params: 4,574,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 171.18\n",
      "==========================================================================================\n",
      "Input size (MB): 144.00\n",
      "Forward/backward pass size (MB): 2558.90\n",
      "Params size (MB): 18.30\n",
      "Estimated Total Size (MB): 2721.20\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = LipNet(num_classes=VOCAB_SIZE + 1).to(DEVICE) # +1 for blank\n",
    "\n",
    "# --- Optional: Print Model Summary (requires torchinfo) ---\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "    # Input shape: (N, T, H, W, C)\n",
    "    print(summary(model, input_size=(BATCH_SIZE_TRAIN, FRAME_COUNT, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS)))\n",
    "except ImportError:\n",
    "    print(\"torchinfo not installed. Skipping model summary.\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "884339bd-d7d2-4b0f-9a4f-7a1e8eaad87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loss Function ---\n",
    "# reduction='mean' averages loss over the batch\n",
    "# zero_infinity=True helps prevent NaN/inf gradients if loss explodes\n",
    "ctc_loss = nn.CTCLoss(blank=CTC_BLANK_INDEX, reduction='mean', zero_infinity=True)\n",
    "\n",
    "# --- Optimizer ---\n",
    "LEARNING_RATE = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723e00e2-f449-4abb-b93d-a296fe8da20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Checkpoint Path ---\n",
    "checkpoint_dir = 'models_pytorch'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'lipnet_checkpoint.pth')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c134986-8e30-41cb-a21d-11974017a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Greedy CTC Decoder (for ProduceExample) ---\n",
    "def greedy_decoder(log_probs, input_lengths):\n",
    "    decoded_sequences = []\n",
    "    # log_probs shape: (T, N, C)\n",
    "    # input_lengths shape: (N,)\n",
    "    for i in range(log_probs.size(1)): # Iterate through batch\n",
    "        # Get relevant log_probs for this sample based on its input length\n",
    "        sample_log_probs = log_probs[:input_lengths[i], i, :]\n",
    "        # Get the best class index at each time step\n",
    "        best_path = torch.argmax(sample_log_probs, dim=1)\n",
    "        # Collapse repeated labels and remove blanks\n",
    "        decoded = []\n",
    "        last_char = -1\n",
    "        for char_idx in best_path:\n",
    "            idx = char_idx.item()\n",
    "            if idx != last_char and idx != CTC_BLANK_INDEX:\n",
    "                decoded.append(idx)\n",
    "            if idx != CTC_BLANK_INDEX: # Update last_char only if it's not blank\n",
    "                last_char = idx\n",
    "        decoded_sequences.append(decoded)\n",
    "    return decoded_sequences\n",
    "\n",
    "# --- ProduceExample Function (Replaces Keras Callback) ---\n",
    "def produce_example(model, dataset_loader, num_to_char_map):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Get one batch from the provided loader (e.g., test_loader.take(1))\n",
    "            frames_batch, labels_batch, input_lengths, label_lengths = next(iter(dataset_loader))\n",
    "            frames_batch = frames_batch.to(DEVICE)\n",
    "            # No need to move labels/lengths to GPU for this function\n",
    "\n",
    "            log_probs = model(frames_batch) # Shape (T, N, C)\n",
    "            # Move log_probs back to CPU for decoding if necessary, ensure input_lengths is CPU tensor\n",
    "            log_probs_cpu = log_probs.cpu()\n",
    "            input_lengths_cpu = input_lengths.cpu()\n",
    "\n",
    "            # Greedy decoding\n",
    "            decoded_indices_list = greedy_decoder(log_probs_cpu, input_lengths_cpu)\n",
    "\n",
    "            print(\"\\n--- Example Predictions ---\")\n",
    "            N_EXAMPLES_TO_SHOW = min(2, frames_batch.size(0))\n",
    "            for i in range(N_EXAMPLES_TO_SHOW):\n",
    "                # Original Label\n",
    "                original_indices = labels_batch[i][:label_lengths[i]].tolist()\n",
    "                original_text = \"\".join([num_to_char_map.get(idx, '?') for idx in original_indices])\n",
    "                print(f'Original:     {original_text}')\n",
    "\n",
    "                # Prediction\n",
    "                prediction_indices = decoded_indices_list[i]\n",
    "                print(f'Filtered Idx: {prediction_indices}') # For debugging\n",
    "                prediction_text = \"\".join([num_to_char_map.get(idx, '?') for idx in prediction_indices])\n",
    "                print(f'Prediction:   {prediction_text}')\n",
    "                print('-'*50)\n",
    "            print(\"--- End Examples ---\\n\")\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"Warning: Could not get a batch from the dataset loader for ProduceExample.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ProduceExample: {e}\")\n",
    "    model.train() # Set model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1edb1f6-c6be-4826-a7f8-2f2cc37043e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from models_pytorch/lipnet_checkpoint.pth\n",
      "Resuming training from epoch 10, Best loss: 5.6313\n",
      "\n",
      "Starting Training...\n",
      "Training batches per epoch: 25\n",
      "Validation batches per epoch: 7\n",
      "\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train: 100%|██████████| 25/25 [00:15<00:00,  1.60it/s, loss=3.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training Loss: 3.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 Val: 100%|██████████| 7/7 [00:02<00:00,  3.24it/s, val_loss=5.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Loss: 5.6227\n",
      "Validation loss improved from 5.6313 to 5.6227, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train: 100%|██████████| 25/25 [00:15<00:00,  1.58it/s, loss=3.48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training Loss: 3.4782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 Val: 100%|██████████| 7/7 [00:02<00:00,  3.06it/s, val_loss=4.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Loss: 4.9936\n",
      "Validation loss improved from 5.6227 to 4.9936, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=3.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training Loss: 3.3737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 Val: 100%|██████████| 7/7 [00:02<00:00,  3.10it/s, val_loss=5.07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Loss: 5.0674\n",
      "Validation loss (5.0674) did not improve from 4.9936\n",
      "\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=3.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training Loss: 3.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Val: 100%|██████████| 7/7 [00:02<00:00,  3.17it/s, val_loss=4.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Loss: 4.5019\n",
      "Validation loss improved from 4.9936 to 4.5019, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=3.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training Loss: 3.1715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Val: 100%|██████████| 7/7 [00:02<00:00,  3.24it/s, val_loss=4.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Validation Loss: 4.3916\n",
      "Validation loss improved from 4.5019 to 4.3916, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=3.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training Loss: 3.1084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 Val: 100%|██████████| 7/7 [00:02<00:00,  3.28it/s, val_loss=3.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Validation Loss: 3.8554\n",
      "Validation loss improved from 4.3916 to 3.8554, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=3.05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training Loss: 3.0481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 Val: 100%|██████████| 7/7 [00:02<00:00,  3.17it/s, val_loss=3.91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Validation Loss: 3.9138\n",
      "Validation loss (3.9138) did not improve from 3.8554\n",
      "\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=3.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training Loss: 3.0094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 Val: 100%|██████████| 7/7 [00:02<00:00,  3.18it/s, val_loss=3.96]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Validation Loss: 3.9615\n",
      "Validation loss (3.9615) did not improve from 3.8554\n",
      "\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.99]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training Loss: 2.9869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 Val: 100%|██████████| 7/7 [00:02<00:00,  2.95it/s, val_loss=3.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Validation Loss: 3.5453\n",
      "Validation loss improved from 3.8554 to 3.5453, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train: 100%|██████████| 25/25 [00:15<00:00,  1.58it/s, loss=2.96]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training Loss: 2.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 Val: 100%|██████████| 7/7 [00:02<00:00,  3.11it/s, val_loss=3.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Validation Loss: 3.3019\n",
      "Validation loss improved from 3.5453 to 3.3019, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "--- Example Predictions ---\n",
      "Original:     place green with r seven again\n",
      "Filtered Idx: []\n",
      "Prediction:   \n",
      "--------------------------------------------------\n",
      "Original:     bin green with u nine again\n",
      "Filtered Idx: []\n",
      "Prediction:   \n",
      "--------------------------------------------------\n",
      "--- End Examples ---\n",
      "\n",
      "\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Train: 100%|██████████| 25/25 [00:15<00:00,  1.56it/s, loss=2.91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Training Loss: 2.9064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 Val: 100%|██████████| 7/7 [00:02<00:00,  3.14it/s, val_loss=3.5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Validation Loss: 3.4981\n",
      "Validation loss (3.4981) did not improve from 3.3019\n",
      "\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.89]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Training Loss: 2.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 Val: 100%|██████████| 7/7 [00:02<00:00,  3.12it/s, val_loss=3.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Validation Loss: 3.4335\n",
      "Validation loss (3.4335) did not improve from 3.3019\n",
      "\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.87]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Training Loss: 2.8676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 Val: 100%|██████████| 7/7 [00:02<00:00,  3.10it/s, val_loss=3.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Validation Loss: 3.4461\n",
      "Validation loss (3.4461) did not improve from 3.3019\n",
      "\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Training Loss: 2.8418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 Val: 100%|██████████| 7/7 [00:02<00:00,  3.12it/s, val_loss=3.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Validation Loss: 3.2002\n",
      "Validation loss improved from 3.3019 to 3.2002, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Train: 100%|██████████| 25/25 [00:15<00:00,  1.56it/s, loss=2.83]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Training Loss: 2.8291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 Val: 100%|██████████| 7/7 [00:02<00:00,  3.05it/s, val_loss=3.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Validation Loss: 3.0337\n",
      "Validation loss improved from 3.2002 to 3.0337, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.82]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Training Loss: 2.8239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 Val: 100%|██████████| 7/7 [00:02<00:00,  3.20it/s, val_loss=3.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Validation Loss: 3.2514\n",
      "Validation loss (3.2514) did not improve from 3.0337\n",
      "\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Training Loss: 2.8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 Val: 100%|██████████| 7/7 [00:02<00:00,  2.86it/s, val_loss=2.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Validation Loss: 2.9733\n",
      "Validation loss improved from 3.0337 to 2.9733, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Training Loss: 2.7907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 Val: 100%|██████████| 7/7 [00:02<00:00,  3.18it/s, val_loss=3.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Validation Loss: 3.3320\n",
      "Validation loss (3.3320) did not improve from 2.9733\n",
      "\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 Train: 100%|██████████| 25/25 [00:16<00:00,  1.54it/s, loss=2.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Training Loss: 2.7892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 Val: 100%|██████████| 7/7 [00:02<00:00,  3.17it/s, val_loss=3.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Validation Loss: 3.0807\n",
      "Validation loss (3.0807) did not improve from 2.9733\n",
      "\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Training Loss: 2.7744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 Val: 100%|██████████| 7/7 [00:02<00:00,  3.11it/s, val_loss=3.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Validation Loss: 3.0170\n",
      "Validation loss (3.0170) did not improve from 2.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Predictions ---\n",
      "Original:     place green with r seven again\n",
      "Filtered Idx: []\n",
      "Prediction:   \n",
      "--------------------------------------------------\n",
      "Original:     bin green with u nine again\n",
      "Filtered Idx: []\n",
      "Prediction:   \n",
      "--------------------------------------------------\n",
      "--- End Examples ---\n",
      "\n",
      "\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Training Loss: 2.7570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 Val: 100%|██████████| 7/7 [00:02<00:00,  3.08it/s, val_loss=2.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Validation Loss: 2.9103\n",
      "Validation loss improved from 2.9733 to 2.9103, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Training Loss: 2.7511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 Val: 100%|██████████| 7/7 [00:02<00:00,  3.24it/s, val_loss=2.92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Validation Loss: 2.9248\n",
      "Validation loss (2.9248) did not improve from 2.9103\n",
      "\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 Train: 100%|██████████| 25/25 [00:15<00:00,  1.56it/s, loss=2.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Training Loss: 2.7373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 Val: 100%|██████████| 7/7 [00:02<00:00,  3.05it/s, val_loss=2.92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Validation Loss: 2.9154\n",
      "Validation loss (2.9154) did not improve from 2.9103\n",
      "\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Training Loss: 2.7318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 Val: 100%|██████████| 7/7 [00:02<00:00,  3.19it/s, val_loss=2.94]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Validation Loss: 2.9429\n",
      "Validation loss (2.9429) did not improve from 2.9103\n",
      "\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Training Loss: 2.7259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 Val: 100%|██████████| 7/7 [00:02<00:00,  3.07it/s, val_loss=2.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Validation Loss: 2.8818\n",
      "Validation loss improved from 2.9103 to 2.8818, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Training Loss: 2.7134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 Val: 100%|██████████| 7/7 [00:02<00:00,  3.07it/s, val_loss=2.95]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Validation Loss: 2.9488\n",
      "Validation loss (2.9488) did not improve from 2.8818\n",
      "\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Training Loss: 2.7116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 Val: 100%|██████████| 7/7 [00:02<00:00,  3.14it/s, val_loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Validation Loss: 2.8166\n",
      "Validation loss improved from 2.8818 to 2.8166, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Training Loss: 2.7112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 Val: 100%|██████████| 7/7 [00:02<00:00,  3.07it/s, val_loss=2.92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Validation Loss: 2.9198\n",
      "Validation loss (2.9198) did not improve from 2.8166\n",
      "\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Training Loss: 2.6989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 Val: 100%|██████████| 7/7 [00:02<00:00,  2.93it/s, val_loss=2.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Validation Loss: 2.7876\n",
      "Validation loss improved from 2.8166 to 2.7876, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Training Loss: 2.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 Val: 100%|██████████| 7/7 [00:02<00:00,  3.15it/s, val_loss=2.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Validation Loss: 2.7989\n",
      "Validation loss (2.7989) did not improve from 2.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Predictions ---\n",
      "Original:     place green with r seven again\n",
      "Filtered Idx: []\n",
      "Prediction:   \n",
      "--------------------------------------------------\n",
      "Original:     bin green with u nine again\n",
      "Filtered Idx: []\n",
      "Prediction:   \n",
      "--------------------------------------------------\n",
      "--- End Examples ---\n",
      "\n",
      "\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Training Loss: 2.6848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41 Val: 100%|██████████| 7/7 [00:02<00:00,  3.07it/s, val_loss=2.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Validation Loss: 2.8135\n",
      "Validation loss (2.8135) did not improve from 2.7876\n",
      "\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Training Loss: 2.6854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42 Val: 100%|██████████| 7/7 [00:02<00:00,  2.97it/s, val_loss=2.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Validation Loss: 2.7406\n",
      "Validation loss improved from 2.7876 to 2.7406, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Training Loss: 2.6812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43 Val: 100%|██████████| 7/7 [00:02<00:00,  3.06it/s, val_loss=2.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Validation Loss: 2.7889\n",
      "Validation loss (2.7889) did not improve from 2.7406\n",
      "\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Training Loss: 2.6790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44 Val: 100%|██████████| 7/7 [00:02<00:00,  3.20it/s, val_loss=2.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Validation Loss: 2.7657\n",
      "Validation loss (2.7657) did not improve from 2.7406\n",
      "\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Training Loss: 2.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45 Val: 100%|██████████| 7/7 [00:02<00:00,  3.21it/s, val_loss=2.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Validation Loss: 2.7765\n",
      "Validation loss (2.7765) did not improve from 2.7406\n",
      "\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Training Loss: 2.6653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46 Val: 100%|██████████| 7/7 [00:02<00:00,  3.19it/s, val_loss=2.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Validation Loss: 2.7057\n",
      "Validation loss improved from 2.7406 to 2.7057, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.66]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Training Loss: 2.6622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47 Val: 100%|██████████| 7/7 [00:02<00:00,  3.12it/s, val_loss=2.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Validation Loss: 2.7294\n",
      "Validation loss (2.7294) did not improve from 2.7057\n",
      "\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Training Loss: 2.6518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 Val: 100%|██████████| 7/7 [00:02<00:00,  3.01it/s, val_loss=2.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Validation Loss: 2.7878\n",
      "Validation loss (2.7878) did not improve from 2.7057\n",
      "\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Training Loss: 2.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49 Val: 100%|██████████| 7/7 [00:02<00:00,  3.13it/s, val_loss=2.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Validation Loss: 2.7266\n",
      "Validation loss (2.7266) did not improve from 2.7057\n",
      "\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Training Loss: 2.6405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50 Val: 100%|██████████| 7/7 [00:02<00:00,  3.03it/s, val_loss=2.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Validation Loss: 2.6435\n",
      "Validation loss improved from 2.7057 to 2.6435, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "--- Example Predictions ---\n",
      "Original:     place green with r seven again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "Original:     bin green with u nine again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "--- End Examples ---\n",
      "\n",
      "\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Training Loss: 2.6451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51 Val: 100%|██████████| 7/7 [00:02<00:00,  2.92it/s, val_loss=2.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Validation Loss: 2.6880\n",
      "Validation loss (2.6880) did not improve from 2.6435\n",
      "\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Training Loss: 2.6410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52 Val: 100%|██████████| 7/7 [00:02<00:00,  3.10it/s, val_loss=2.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Validation Loss: 2.6793\n",
      "Validation loss (2.6793) did not improve from 2.6435\n",
      "\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53 Train: 100%|██████████| 25/25 [00:15<00:00,  1.56it/s, loss=2.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Training Loss: 2.6314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53 Val: 100%|██████████| 7/7 [00:02<00:00,  3.17it/s, val_loss=2.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Validation Loss: 2.6325\n",
      "Validation loss improved from 2.6435 to 2.6325, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Training Loss: 2.6268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54 Val: 100%|██████████| 7/7 [00:02<00:00,  3.12it/s, val_loss=2.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Validation Loss: 2.6900\n",
      "Validation loss (2.6900) did not improve from 2.6325\n",
      "\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Training Loss: 2.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55 Val: 100%|██████████| 7/7 [00:02<00:00,  3.13it/s, val_loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Validation Loss: 2.6528\n",
      "Validation loss (2.6528) did not improve from 2.6325\n",
      "\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Training Loss: 2.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56 Val: 100%|██████████| 7/7 [00:02<00:00,  3.11it/s, val_loss=2.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Validation Loss: 2.6415\n",
      "Validation loss (2.6415) did not improve from 2.6325\n",
      "\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Training Loss: 2.6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57 Val: 100%|██████████| 7/7 [00:02<00:00,  3.14it/s, val_loss=2.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Validation Loss: 2.7416\n",
      "Validation loss (2.7416) did not improve from 2.6325\n",
      "\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Training Loss: 2.6126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58 Val: 100%|██████████| 7/7 [00:02<00:00,  2.93it/s, val_loss=2.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Validation Loss: 2.6130\n",
      "Validation loss improved from 2.6325 to 2.6130, saving model to models_pytorch/lipnet_checkpoint.pth\n",
      "\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Training Loss: 2.6183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59 Val: 100%|██████████| 7/7 [00:02<00:00,  2.85it/s, val_loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Validation Loss: 2.6523\n",
      "Validation loss (2.6523) did not improve from 2.6130\n",
      "\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Training Loss: 2.6026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60 Val: 100%|██████████| 7/7 [00:02<00:00,  3.08it/s, val_loss=2.62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Validation Loss: 2.6223\n",
      "Validation loss (2.6223) did not improve from 2.6130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Predictions ---\n",
      "Original:     place green with r seven again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "Original:     bin green with u nine again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "--- End Examples ---\n",
      "\n",
      "\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61 Train: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s, loss=2.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Training Loss: 2.6009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61 Val: 100%|██████████| 7/7 [00:02<00:00,  3.10it/s, val_loss=2.66]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Validation Loss: 2.6563\n",
      "Validation loss (2.6563) did not improve from 2.6130\n",
      "\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62 Train: 100%|██████████| 25/25 [00:16<00:00,  1.54it/s, loss=2.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Training Loss: 2.6037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62 Val: 100%|██████████| 7/7 [00:02<00:00,  3.17it/s, val_loss=2.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Validation Loss: 2.6399\n",
      "Validation loss (2.6399) did not improve from 2.6130\n",
      "\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Training Loss: 2.5954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63 Val: 100%|██████████| 7/7 [00:02<00:00,  3.06it/s, val_loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Validation Loss: 2.6515\n",
      "Validation loss (2.6515) did not improve from 2.6130\n",
      "\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Training Loss: 2.5963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64 Val: 100%|██████████| 7/7 [00:02<00:00,  3.21it/s, val_loss=2.66]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Validation Loss: 2.6643\n",
      "Validation loss (2.6643) did not improve from 2.6130\n",
      "\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65 Train: 100%|██████████| 25/25 [00:15<00:00,  1.56it/s, loss=2.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Training Loss: 2.5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65 Val: 100%|██████████| 7/7 [00:02<00:00,  3.19it/s, val_loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Validation Loss: 2.6503\n",
      "Validation loss (2.6503) did not improve from 2.6130\n",
      "\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66 Train: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s, loss=2.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Training Loss: 2.5864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66 Val: 100%|██████████| 7/7 [00:02<00:00,  3.15it/s, val_loss=2.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Validation Loss: 2.6871\n",
      "Validation loss (2.6871) did not improve from 2.6130\n",
      "\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67 Train: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s, loss=2.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Training Loss: 2.5861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67 Val: 100%|██████████| 7/7 [00:02<00:00,  2.94it/s, val_loss=2.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Validation Loss: 2.6265\n",
      "Validation loss (2.6265) did not improve from 2.6130\n",
      "\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68 Train:  44%|████▍     | 11/25 [00:08<00:10,  1.33it/s, loss=2.59]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m      \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip gradient update for this batch\u001b[39;00m\n\u001b[32m     64\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m running_loss += loss.item()\n\u001b[32m     68\u001b[39m train_pbar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: running_loss / (i + \u001b[32m1\u001b[39m)})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/adam.py:685\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    682\u001b[39m     torch._foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    684\u001b[39m     bias_correction1 = [\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m         \u001b[32m1\u001b[39m - beta1 ** \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    686\u001b[39m     ]\n\u001b[32m    687\u001b[39m     bias_correction2 = [\n\u001b[32m    688\u001b[39m         \u001b[32m1\u001b[39m - beta2 ** _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    689\u001b[39m     ]\n\u001b[32m    691\u001b[39m     step_size = _stack_if_compiling([(lr / bc) * -\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/optim/optimizer.py:106\u001b[39m, in \u001b[36m_get_value\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "EPOCHS = 100 # As in TF code\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Optional: Load checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['loss']\n",
    "        print(f\"Resuming training from epoch {start_epoch}, Best loss: {best_val_loss:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
    "        start_epoch = 0\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "num_train_batches = len(train_loader)\n",
    "num_test_batches = len(test_loader)\n",
    "print(f\"Training batches per epoch: {num_train_batches}\")\n",
    "print(f\"Validation batches per epoch: {num_test_batches}\")\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_pbar = tqdm(enumerate(train_loader), total=num_train_batches, desc=f\"Epoch {epoch+1} Train\")\n",
    "\n",
    "    for i, batch_data in train_pbar:\n",
    "        frames_batch, labels_padded, input_lengths, label_lengths = batch_data\n",
    "\n",
    "        # Move data to device\n",
    "        frames_batch = frames_batch.to(DEVICE)\n",
    "        labels_padded = labels_padded.to(DEVICE)\n",
    "        input_lengths = input_lengths.to(DEVICE) # Though lengths often stay CPU for CTC\n",
    "        label_lengths = label_lengths.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass -> Logits (T, N, C)\n",
    "        log_probs = model(frames_batch)\n",
    "\n",
    "        # Ensure input lengths don't exceed T\n",
    "        T = log_probs.size(0)\n",
    "        input_lengths = torch.clamp(input_lengths, max=T)\n",
    "\n",
    "        # Calculate CTC Loss\n",
    "        # Ensure labels_padded, input_lengths, label_lengths are on CPU if required by CTCLoss implementation\n",
    "        loss = ctc_loss(log_probs, labels_padded.cpu(), input_lengths.cpu(), label_lengths.cpu())\n",
    "\n",
    "        # Check for NaN/inf loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "             print(f\"Warning: NaN or Inf loss detected at batch {i}. Skipping batch.\")\n",
    "             continue # Skip gradient update for this batch\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_pbar.set_postfix({'loss': running_loss / (i + 1)})\n",
    "\n",
    "    avg_train_loss = running_loss / num_train_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_pbar = tqdm(enumerate(test_loader), total=num_test_batches, desc=f\"Epoch {epoch+1} Val\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in val_pbar:\n",
    "            frames_batch, labels_padded, input_lengths, label_lengths = batch_data\n",
    "            frames_batch = frames_batch.to(DEVICE)\n",
    "            labels_padded = labels_padded.to(DEVICE)\n",
    "            input_lengths = input_lengths.to(DEVICE)\n",
    "            label_lengths = label_lengths.to(DEVICE)\n",
    "\n",
    "            log_probs = model(frames_batch)\n",
    "            T = log_probs.size(0)\n",
    "            input_lengths = torch.clamp(input_lengths, max=T)\n",
    "\n",
    "            loss = ctc_loss(log_probs, labels_padded.cpu(), input_lengths.cpu(), label_lengths.cpu())\n",
    "\n",
    "            if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                running_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'val_loss': running_val_loss / (i + 1)})\n",
    "\n",
    "    avg_val_loss = running_val_loss / num_test_batches\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # --- Save Checkpoint ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}, saving model to {checkpoint_path}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_val_loss,\n",
    "        }, checkpoint_path)\n",
    "        best_val_loss = avg_val_loss\n",
    "    else:\n",
    "        print(f\"Validation loss ({avg_val_loss:.4f}) did not improve from {best_val_loss:.4f}\")\n",
    "\n",
    "    # --- Show Examples (e.g., every 5 epochs) ---\n",
    "    if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:\n",
    "         produce_example(model, test_loader, num_to_char_dict)\n",
    "\n",
    "print(\"\\nTraining Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "033fbf86-b8f4-496e-a34d-d2efc7ebd1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Final Test Examples ---\n",
      "Loading best weights from models_pytorch/lipnet_checkpoint.pth for final test...\n",
      "Best weights loaded successfully.\n",
      "\n",
      "--- Example Predictions ---\n",
      "Original:     place green with r seven again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "Original:     bin green with u nine again\n",
      "Filtered Idx: [14]\n",
      "Prediction:   n\n",
      "--------------------------------------------------\n",
      "--- End Examples ---\n",
      "\n",
      "--- Final Testing Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Plotting Training/Validation Loss ---\n",
    "plt.figure(figsize=(8, 3))\n",
    "# plt.plot(range(1, (EPOCHS - start_epoch) + 1), train_losses, color='red', label=\"Training Loss\")\n",
    "# plt.plot(range(1, (EPOCHS - start_epoch) + 1), val_losses, color='green', label=\"Validation Loss\")\n",
    "plt.plot(train_losses, color='red', label=\"Training Loss\")\n",
    "plt.plot(val_losses, color='green', label=\"Validation Loss\")\n",
    "plt.title(\"Training/Validation Loss vs Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_curve_pytorch.png\")\n",
    "plt.show()\n",
    "\n",
    "# --- Final Testing Example (similar to TF code) ---\n",
    "print(\"\\n--- Running Final Test Examples ---\")\n",
    "# Load best weights\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading best weights from {checkpoint_path} for final test...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Best weights loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best weights: {e}\")\n",
    "else:\n",
    "    print(\"No checkpoint found for final testing.\")\n",
    "\n",
    "produce_example(model, test_loader, num_to_char_dict) # Show examples with best model\n",
    "print(\"--- Final Testing Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ade41079-8532-4bcf-a517-606be35085cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking .npy files in ./processed_mouth_data/...\n",
      "Found 1000 files. Checking shapes...\n",
      "PROBLEM: ./processed_mouth_data/s1/lrae3s_mouth.npy has shape (74, 50, 100, 3)\n",
      "PROBLEM: ./processed_mouth_data/s1/sbbbzp_mouth.npy has shape (74, 50, 100, 3)\n",
      "PROBLEM: ./processed_mouth_data/s1/srbb4n_mouth.npy has shape (74, 50, 100, 3)\n",
      "PROBLEM: ./processed_mouth_data/s1/srwi5a_mouth.npy has shape (74, 50, 100, 3)\n",
      "PROBLEM: ./processed_mouth_data/s1/swao7a_mouth.npy has shape (74, 50, 100, 3)\n",
      "\n",
      "Found 5 files with potential shape issues.\n",
      "Problematic files removed. Please re-run preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# BASE_PROCESSED_PATH = './processed_mouth_data/'\n",
    "# FRAME_HEIGHT = 50\n",
    "# FRAME_WIDTH = 100\n",
    "# FRAME_CHANNELS = 3\n",
    "# TARGET_SHAPE_LAST_DIMS = (75, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS)\n",
    "\n",
    "# print(f\"Checking .npy files in {BASE_PROCESSED_PATH}...\")\n",
    "# problem_files = []\n",
    "# all_files = glob.glob(os.path.join(BASE_PROCESSED_PATH, 's*', '*.npy'))\n",
    "\n",
    "# if not all_files:\n",
    "#     print(\"No .npy files found. Make sure BASE_PROCESSED_PATH is correct.\")\n",
    "# else:\n",
    "#     print(f\"Found {len(all_files)} files. Checking shapes...\")\n",
    "#     for npy_file in all_files:\n",
    "#         try:\n",
    "#             data = np.load(npy_file)\n",
    "#             if data.ndim != 4 or data.shape[:] != TARGET_SHAPE_LAST_DIMS:\n",
    "#                 print(f\"PROBLEM: {npy_file} has shape {data.shape}\")\n",
    "#                 problem_files.append(npy_file)\n",
    "#         except Exception as e:\n",
    "#             print(f\"ERROR reading {npy_file}: {e}\")\n",
    "#             problem_files.append(npy_file)\n",
    "\n",
    "#     if not problem_files:\n",
    "#         print(\"All checked .npy files seem to have correct H, W, C dimensions.\")\n",
    "#     else:\n",
    "#         print(f\"\\nFound {len(problem_files)} files with potential shape issues.\")\n",
    "#         # Optionally, you could delete or reprocess these problem files\n",
    "#         for f_path in problem_files:\n",
    "#             os.remove(f_path)\n",
    "#         print(\"Problematic files removed. Please re-run preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c6b21-846a-4c2b-b2f1-7d15f749b46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
