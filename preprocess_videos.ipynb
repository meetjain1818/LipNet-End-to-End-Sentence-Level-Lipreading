{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547a1c65-0bb2-42f8-8132-0541bb688651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/jedidiahangekouakou/grid-corpus-dataset-for-training-lipnet?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12.6G/12.6G [01:20<00:00, 168MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /teamspace/studios/this_studio/.cache/kagglehub/datasets/jedidiahangekouakou/grid-corpus-dataset-for-training-lipnet/versions/1\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"jedidiahangekouakou/grid-corpus-dataset-for-training-lipnet\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749d5270-043f-4e66-8a5a-c7e08ca84fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional\n",
    "import imageio\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1f6786-cd26-49c4-8dd0-f419184505b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SPEAKER_IDS = [f's{i}_processed' for i in range(1, 5) if i != 21]\n",
    "BASE_PROCESSED_PATH = './GRIDCorpus/processed_mouth_data/'\n",
    "FRAME_COUNT = 75\n",
    "FRAME_HEIGHT = 50\n",
    "FRAME_WIDTH = 100\n",
    "FRAME_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655fa55a-2bd4-40cc-8514-d635f498960c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s1_processed', 's2_processed', 's3_processed', 's4_processed']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_SPEAKER_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f6d26-5145-430e-92fc-9285987bc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mouth Extraction (Mostly unchanged, ensure return type is numpy) ---\n",
    "try:\n",
    "    DLIB_LANDMARK_PREDICTOR = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    if not os.path.exists(DLIB_LANDMARK_PREDICTOR):\n",
    "        # Add download/unzip logic here if needed, e.g., using requests/bz2\n",
    "        print(f\"Error: dlib landmark predictor '{DLIB_LANDMARK_PREDICTOR}' not found.\")\n",
    "        print(\"Please download it from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "        exit()\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(DLIB_LANDMARK_PREDICTOR)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing dlib: {e}\")\n",
    "    print(\"Make sure dlib is installed correctly and the predictor file exists.\")\n",
    "    exit()\n",
    "\n",
    "def extract_mouth_region(frame: np.ndarray) -> Optional[np.ndarray]:\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    face = faces[0]\n",
    "    landmarks = predictor(gray, face)\n",
    "    points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)])\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(points)\n",
    "    # Adjust margins carefully - TF code used slightly different logic\n",
    "    # Let's try to match the TF code's effective crop:\n",
    "    y_start = max(y + 15 - 30, 0) # y + 15 was start, margin was 30\n",
    "    y_end = y + 15 + h + 30\n",
    "    x_start = max(x + 15 - 30, 0) # x + 15 was start, margin was 30\n",
    "    x_end = x + 15 + w + 30\n",
    "\n",
    "    # Ensure coordinates are within frame bounds\n",
    "    y_start = max(0, y_start)\n",
    "    y_end = min(frame.shape[0], y_end)\n",
    "    x_start = max(0, x_start)\n",
    "    x_end = min(frame.shape[1], x_end)\n",
    "\n",
    "    cropped = frame[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    if cropped.size == 0: # Handle empty crop\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "    try:\n",
    "        cropped = cv2.resize(cropped, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "        return cropped\n",
    "    except cv2.error as e:\n",
    "        print(f\"Warning: cv2.resize error ({e}). Returning zero frame.\")\n",
    "        return np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "def process_and_save_video(video_path: str, output_dir: str) -> None:\n",
    "    \"\"\"Processes a single video, extracts mouth regions, and saves as .npy\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening video file: {video_path}\")\n",
    "            return None\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            mouth_frame = extract_mouth_region(frame)\n",
    "            if mouth_frame is not None:\n",
    "                frames.append(mouth_frame)\n",
    "            else:\n",
    "                # If detection fails, append a zero frame\n",
    "                frames.append(np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8))\n",
    "        cap.release()\n",
    "\n",
    "        if not frames:\n",
    "            print(f\"Warning: No frames extracted from {video_path}\")\n",
    "            return None\n",
    "\n",
    "        # Ensure video has FRAME_COUNT frames (pad/truncate if needed)\n",
    "        frames_np = np.array(frames, dtype=np.uint8) # Keep as uint8 for now\n",
    "        current_frame_count = frames_np.shape[0]\n",
    "        if current_frame_count != FRAME_COUNT:\n",
    "             if current_frame_count > FRAME_COUNT:\n",
    "                 frames_np = frames_np[:FRAME_COUNT, ...]\n",
    "             else:\n",
    "                 pad_width = ((0, FRAME_COUNT - current_frame_count), (0, 0), (0, 0), (0, 0))\n",
    "                 frames_np = np.pad(frames_np, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "        video_id = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        output_path = os.path.join(output_dir, f\"{video_id}_mouth.npy\")\n",
    "        np.save(output_path, frames_np)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Function to preprocess all videos (Run only once) ---\n",
    "def preprocess_all_videos(force_reprocess=False):\n",
    "    print(\"Starting video preprocessing...\")\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    for speaker_id in ALL_SPEAKER_IDS:\n",
    "        input_videos_dir = os.path.join('./GRIDCorpus/data/', speaker_id) # Assuming video folder structure\n",
    "        output_preprocessed_dir = os.path.join(BASE_PROCESSED_PATH, speaker_id)\n",
    "        os.makedirs(output_preprocessed_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing speaker: {speaker_id}\")\n",
    "        video_files = glob.glob(os.path.join(input_videos_dir, '*.mpg')) # Assuming .mpg format\n",
    "\n",
    "        if not video_files:\n",
    "             print(f\"  Warning: No .mpg files found in {input_videos_dir}\")\n",
    "             continue\n",
    "\n",
    "        for video_file in tqdm(video_files, desc=f\"Speaker {speaker_id}\", unit=\"video\"):\n",
    "             video_id = os.path.splitext(os.path.basename(video_file))[0]\n",
    "             output_path = os.path.join(output_preprocessed_dir, f\"{video_id}_mouth.npy\")\n",
    "             if not force_reprocess and os.path.exists(output_path):\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "             process_and_save_video(video_file, output_preprocessed_dir)\n",
    "             processed_count += 1\n",
    "\n",
    "    print(f\"\\nPreprocessing finished. Processed: {processed_count}, Skipped (already exists): {skipped_count}\")\n",
    "\n",
    "# --- UNCOMMENT AND RUN THIS ONCE TO PREPROCESS ---\n",
    "preprocess_all_videos(force_reprocess=False)\n",
    "print(\"Preprocessing complete.\")\n",
    "# --- END PREPROCESSING CALL ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3230bba0-5321-45c0-b22f-057e9a86cf9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwalk\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./GRIDCorpus/processed_mouth_data/s3_processed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "len(os.walk(\"./GRIDCorpus/processed_mouth_data/s3_processed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05294c29-d965-42cd-abe7-9bc4b92f8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = os.walk(\"./GRIDCorpus/processed_mouth_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed13f92-b4b6-403d-8d71-ffd8edeffd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object walk at 0x74cc8113f400>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44fc18e9-37f1-4a6b-bc7e-67dcf2287364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./GRIDCorpus/processed_mouth_data/',\n",
       " ['s2_processed', 's3_processed', 's1_processed'],\n",
       " [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c11cfe8-79b9-42de-a354-ac763054d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "files_names = []\n",
    "for parent, _, files in iterator:\n",
    "    print(files_names.append(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8496f7e2-6494-4723-abb0-bb4beafed0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859998d-a28b-4281-baa4-b3f834a16533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
